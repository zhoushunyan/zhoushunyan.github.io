<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>PCA | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. 协方差Co">
<meta property="og:type" content="article">
<meta property="og:title" content="PCA">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2019&#x2F;12&#x2F;01&#x2F;PCA%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95&#x2F;index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. 协方差Co">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2019-12-01T05:41:25.222Z">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-PCA线性降维算法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/12/01/PCA%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95/" class="article-date">
  <time datetime="2019-12-01T04:51:06.192Z" itemprop="datePublished">2019-12-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      PCA
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="协方差Cov（x，y）"><a href="#协方差Cov（x，y）" class="headerlink" title="协方差Cov（x，y）"></a>协方差Cov（x，y）</h2><p>COV(X,Y)=E(XY)-E(X)E(Y)<br>COV(X,Y)=[D(X)+D(Y)-D(X+Y)]/2</p>
<h2 id="PCA概念"><a href="#PCA概念" class="headerlink" title="PCA概念"></a>PCA概念</h2><p>PCA 是一种基于从高维空间映射到低维空间的映射方法，也是最基础的无监督降维算法，其目标是向数据变化最大的方向投影，或者说向重构误差最小化的方向投影。它由 Karl Pearson 在 1901 年提出，属于线性降维方法。与 PCA 相关的原理通常被称为最大方差理论或最小误差理论。这两者目标一致，但过程侧重点则不同。</p>
<h2 id="最大方法降维原理"><a href="#最大方法降维原理" class="headerlink" title="最大方法降维原理"></a>最大方法降维原理</h2><p>将一组 N 维向量降为 K 维（K 大于 0，小于 N），其目标是选择 K 个单位正交基，各字段两两间 COV(X,Y) 为 0，而字段的方差则尽可能大。因此，最大方差即使得投影数据的方差被最大化，在这过程中，我们需要找到数据集 Xmxn 的最佳的投影空间 Wnxk、协方差矩阵等，其算法流程为<br>算法输入：数据集 Xmxn；</p>
<p>按列计算数据集 X 的均值 Xmean，然后令 Xnew=XXmean；</p>
<p>求解矩阵 Xnew 的协方差矩阵，并将其记为 Cov；</p>
<p>计算协方差矩阵 COv 的特征值和相应的特征向量；</p>
<p>将特征值按照从大到小的排序，选择其中最大的 k 个，然后将其对应的 k 个特征向量分别作为列向量组成特征向量矩阵 Wnxk；</p>
<p>计算 XnewW，即将数据集 Xnew 投影到选取的特征向量上，这样就得到了我们需要的已经降维的数据集 XnewW。</p>
<h2 id="PCA算法优缺点"><a href="#PCA算法优缺点" class="headerlink" title="PCA算法优缺点"></a>PCA算法优缺点</h2><p>优点：</p>
<p>它是无监督学习，完全无参数限制的。在PCA的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。</p>
<p>用PCA技术可以对数据进行降维，同时对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。</p>
<p>各主成分之间正交，可消除原始数据成分间的相互影响。</p>
<p>计算方法简单，易于在计算机上实现。</p>
<p>缺点：</p>
<p>如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。</p>
<p>贡献率小的主成分往往可能含有对样本差异的重要信息。</p>
<p>特征值矩阵的正交向量空间是否唯一有待讨论。</p>
<p>在非高斯分布的情况下，PCA方法得出的主元可能并不是最优的，此时在寻找主元时不能将方差作为衡量重要性的标准。</p>
<h2 id="python算法实现"><a href="#python算法实现" class="headerlink" title="python算法实现"></a>python算法实现</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.cm <span class="keyword">as</span> cmx</span><br><span class="line"><span class="keyword">import</span> matplotlib.colors <span class="keyword">as</span> colors</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shuffle_data</span><span class="params">(X, y, seed=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> seed:</span><br><span class="line">        np.random.seed(seed)</span><br><span class="line"></span><br><span class="line">    idx = np.arange(X.shape[<span class="number">0</span>])</span><br><span class="line">    np.random.shuffle(idx)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X[idx], y[idx]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 正规化数据集 X</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span><span class="params">(X, axis=<span class="number">-1</span>, p=<span class="number">2</span>)</span>:</span></span><br><span class="line">    lp_norm = np.atleast_1d(np.linalg.norm(X, p, axis))</span><br><span class="line">    lp_norm[lp_norm == <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> X / np.expand_dims(lp_norm, axis)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化数据集 X</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standardize</span><span class="params">(X)</span>:</span></span><br><span class="line">    X_std = np.zeros(X.shape)</span><br><span class="line">    mean = X.mean(axis=<span class="number">0</span>)</span><br><span class="line">    std = X.std(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 做除法运算时请永远记住分母不能等于0的情形</span></span><br><span class="line">    <span class="comment"># X_std = (X - X.mean(axis=0)) / X.std(axis=0) </span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> range(np.shape(X)[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> std[col]:</span><br><span class="line">            X_std[:, col] = (X_std[:, col] - mean[col]) / std[col]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_std</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分数据集为训练集和测试集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_test_split</span><span class="params">(X, y, test_size=<span class="number">0.2</span>, shuffle=True, seed=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        X, y = shuffle_data(X, y, seed)</span><br><span class="line"></span><br><span class="line">    n_train_samples = int(X.shape[<span class="number">0</span>] * (<span class="number">1</span>-test_size))</span><br><span class="line">    x_train, x_test = X[:n_train_samples], X[n_train_samples:]</span><br><span class="line">    y_train, y_test = y[:n_train_samples], y[n_train_samples:]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_train, x_test, y_train, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算矩阵X的协方差矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_covariance_matrix</span><span class="params">(X, Y=np.empty<span class="params">(<span class="params">(<span class="number">0</span>,<span class="number">0</span>)</span>)</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> Y.any():</span><br><span class="line">        Y = X</span><br><span class="line">    n_samples = np.shape(X)[<span class="number">0</span>]</span><br><span class="line">    covariance_matrix = (<span class="number">1</span> / (n_samples<span class="number">-1</span>)) * (X - X.mean(axis=<span class="number">0</span>)).T.dot(Y - Y.mean(axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.array(covariance_matrix, dtype=float)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算数据集X每列的方差</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_variance</span><span class="params">(X)</span>:</span></span><br><span class="line">    n_samples = np.shape(X)[<span class="number">0</span>]</span><br><span class="line">    variance = (<span class="number">1</span> / n_samples) * np.diag((X - X.mean(axis=<span class="number">0</span>)).T.dot(X - X.mean(axis=<span class="number">0</span>)))</span><br><span class="line">    <span class="keyword">return</span> variance</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算数据集X每列的标准差</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_std_dev</span><span class="params">(X)</span>:</span></span><br><span class="line">    std_dev = np.sqrt(calculate_variance(X))</span><br><span class="line">    <span class="keyword">return</span> std_dev</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算相关系数矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_correlation_matrix</span><span class="params">(X, Y=np.empty<span class="params">([<span class="number">0</span>])</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 先计算协方差矩阵</span></span><br><span class="line">    covariance_matrix = calculate_covariance_matrix(X, Y)</span><br><span class="line">    <span class="comment"># 计算X, Y的标准差</span></span><br><span class="line">    std_dev_X = np.expand_dims(calculate_std_dev(X), <span class="number">1</span>)</span><br><span class="line">    std_dev_y = np.expand_dims(calculate_std_dev(Y), <span class="number">1</span>)</span><br><span class="line">    correlation_matrix = np.divide(covariance_matrix, std_dev_X.dot(std_dev_y.T))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.array(correlation_matrix, dtype=float)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PCA</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    主成份分析算法PCA，非监督学习算法.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.eigen_values = <span class="literal">None</span></span><br><span class="line">        self.eigen_vectors = <span class="literal">None</span></span><br><span class="line">        self.k = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" </span></span><br><span class="line"><span class="string">        将原始数据集X通过PCA进行降维</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        covariance = calculate_covariance_matrix(X)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 求解特征值和特征向量</span></span><br><span class="line">        self.eigen_values, self.eigen_vectors = np.linalg.eig(covariance)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将特征值从大到小进行排序，注意特征向量是按列排的，即self.eigen_vectors第k列是self.eigen_values中第k个特征值对应的特征向量</span></span><br><span class="line">        idx = self.eigen_values.argsort()[::<span class="number">-1</span>]</span><br><span class="line">        eigenvalues = self.eigen_values[idx][:self.k]</span><br><span class="line">        eigenvectors = self.eigen_vectors[:, idx][:, :self.k]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将原始数据集X映射到低维空间</span></span><br><span class="line">        X_transformed = X.dot(eigenvectors)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X_transformed</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># Load the dataset</span></span><br><span class="line">    data = datasets.load_iris()</span><br><span class="line">    X = data.data</span><br><span class="line">    y = data.target</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将数据集X映射到低维空间</span></span><br><span class="line">    X_trans = PCA().transform(X)</span><br><span class="line"></span><br><span class="line">    x1 = X_trans[:, <span class="number">0</span>]</span><br><span class="line">    x2 = X_trans[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    cmap = plt.get_cmap(<span class="string">'viridis'</span>)</span><br><span class="line">    colors = [cmap(i) <span class="keyword">for</span> i <span class="keyword">in</span> np.linspace(<span class="number">0</span>, <span class="number">1</span>, len(np.unique(y)))]</span><br><span class="line"></span><br><span class="line">    class_distr = []</span><br><span class="line">    <span class="comment"># Plot the different class distributions</span></span><br><span class="line">    <span class="keyword">for</span> i, l <span class="keyword">in</span> enumerate(np.unique(y)):</span><br><span class="line">        _x1 = x1[y == l]</span><br><span class="line">        _x2 = x2[y == l]</span><br><span class="line">        _y = y[y == l]</span><br><span class="line">        class_distr.append(plt.scatter(_x1, _x2, color=colors[i]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add a legend</span></span><br><span class="line">    plt.legend(class_distr, y, loc=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Axis labels</span></span><br><span class="line">    plt.xlabel(<span class="string">'Principal Component 1'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Principal Component 2'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>


<p>```</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/12/01/PCA%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95/" data-id="ck50nakqy0002c8vjht8qebwk" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/12/01/KPCA%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%20/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          KPCA
        
      </div>
    </a>
  
  
    <a href="/2019/11/07/new-Post/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">new Post</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%87%AA%E8%BF%B0/" rel="tag">自述</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/%E8%87%AA%E8%BF%B0/" style="font-size: 10px;">自述</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/01/06/python%E5%AD%A6%E4%B9%A0/hello-world/">高阶函数</a>
          </li>
        
          <li>
            <a href="/2020/01/05/python%E5%AD%A6%E4%B9%A0/%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7/">高级特性</a>
          </li>
        
          <li>
            <a href="/2019/12/03/MDS%E9%99%8D%E7%BB%B4/">MDS</a>
          </li>
        
          <li>
            <a href="/2019/12/01/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96%E7%AE%97%E6%B3%95/">关键词提取</a>
          </li>
        
          <li>
            <a href="/2019/12/01/KPCA%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%20/">KPCA</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>