<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-关键词提取算法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/12/01/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96%E7%AE%97%E6%B3%95/" class="article-date">
  <time datetime="2019-12-01T07:20:34.587Z" itemprop="datePublished">2019-12-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/01/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96%E7%AE%97%E6%B3%95/">关键词提取</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="技术概要"><a href="#技术概要" class="headerlink" title="技术概要"></a>技术概要</h2><p>描述：如果我们可以准确地将所有的文档都用几个简单的关键词描述出来，我们单单看这几个关键词就可以判断这篇文章是不是我们所需要的。这样就可以提高我们信息获取的效率。</p>
<p>###有监督和无监督<br><strong>有监督</strong>：通过分类的方式进行的，首先，通过构建一个较为丰富和完善的词表，然后通过判断每个文档与词表中的每个词的匹配程度，以这种类似打标签的方法，达到关键词提取的效果。</p>
<p>优点：有较高的精度<br>缺点：需要大批量的标注的数据，人工成不过高。</p>
<p><strong>无监督</strong>：TF-IDF,TextRank算法，主题模型算法（LSA, LSI,LDA)</p>
<h2 id="TF-IDF算法"><a href="#TF-IDF算法" class="headerlink" title="TF/IDF算法"></a>TF/IDF算法</h2><p><strong>TF/IDF算法</strong>（Term Frequency-Inverse Document Frequency),词频-逆文档频次算法。）是一种基于统计的及算法方法。用于评估在一个文档中一个词对某个文档的重要程度。</p>
<p><strong>TF算法：</strong></p>
<p>TF算法是统计一个词在一篇文档中出现的频次，其思想可以这样描述：一个词在文档中出现的次数越多，则对文档的表达能力就越强。<br><img src="https://cdn.mathpix.com/snip/images/eCvWPEPHVDuGuoly8pfa5UysNHY1NvjKl7eDw4gTJgI.original.fullsize.png" alt=""></p>
<p>tf（word）=（word在文档中出现的次数）/（文档总词数）</p>
<p><strong>例子：</strong></p>
<blockquote>
<pre><code>世界献血日，学校团体、献血服务自愿者等可到血液中心参观检验加工过程，我们会对检验结果进行公示，同时血液的价格也会进行公示。</code></pre><p>上文中“献血”，“血液”，“进行”，“公示”等词出现的频次均为2.</p>
</blockquote>
<p><strong>IDF</strong>：<br>统计一个词在文档集中的多少个文档中出现，描述：如果一个词在越少的文档中出现，则其对文档的区分能力就越强<br><img src="https://cdn.mathpix.com/snip/images/xVxByO7dPrxtjRa0HDgYjjUY6g5pcmXsh00JnNvIQLM.original.fullsize.png" alt=""></p>
<p>|D|是文档集中的总文档数 ，|Di|是文档集中出现词i的文档数量。分母加一是采用拉普拉斯平滑，避免有部分新的词没有在语料库中出现过而导致分母为零，增强算法的健壮性。</p>
<p><strong>tfxIdf(i,j)</strong></p>
<p><img src="https://cdn.mathpix.com/snip/images/tyaMpPhnWCSpNJegpmGpjHbsA5-PrNrrKOMtPKAwFEg.original.fullsize.png" alt=""></p>
<p>如上述文档中tf=2/30=0.067。我们假设文档集有1000.其中出现“血液”，“献血”，为15，10.</p>
<p>所以idf（献血）=10  idf（血液）=4.2</p>
<p>##TextRank算法</p>
<p><strong>描述：</strong>TextRank算法则是可以脱离语料库的背景，仅对单个文档进行分析来提取关键词。基于句子的维度分析，给每个句子进行打分，挑出分数最高的n个句子作为文档的关键句。</p>
<p>###PageRank</p>
<p>PageRank算法是Google创始人拉里.佩奇和谢尔盖.布林于1997年构建早期的搜索系统原型是提出的链接分析算法，该算法是他们用来评价搜索系统网页重要性的一种算法。</p>
<p>基本思想：</p>
<p>1）链接数量。一个网页被越多的其他网页链接，说明这个网页越重要。</p>
<p>2）链接质量。一个网页被一个越高权值的网页链接，也能表明这个网页越重要。</p>
<p><img src="https://cdn.mathpix.com/snip/images/TOdNlF5s-sh_JGMr3sTRwPNDp91eu7O4GfJQbdBIlD4.original.fullsize.png" alt=""></p>
<p>算法开始时会将所有网页的得分初始化为1，来对每个网页的分数进行收敛。</p>
<p>上式中，会导致一些孤立网页（也就是没有出入链的网页）可能得分为零。因此加入一个阻尼系数d。</p>
<p><img src="https://cdn.mathpix.com/snip/images/6XV1HV1WBn7X_oBlvwKGzFn_HzHhE4HX_Pt8Z4mwUBU.original.fullsize.png" alt=""></p>
<p>###TextRank<br>TextRank 是在PageRank的基础上修改的。</p>
<p>TextRank进行自动摘要是有权图。因为在计分时除了考虑链接句的重要性外，还要考虑两个句子间的相似性。<br>计算每个句子给它链接句的贡献时，不是通过平均分配的方式，而是通过计算权重占总权重的比例分配的。在这里权重就是两个句子的相似度。</p>
<p>相似度计算可以采用编辑距离，余弦相似度等。</p>
<p><img src="https://cdn.mathpix.com/snip/images/GAbrYgPP7ctBZhLxEkHfzgDsfuiBNGIsgxSgKUCRJdo.original.fullsize.png" alt=""></p>
<p>##主题模型算法</p>
<p>若有一篇文章中介绍了狮子，老虎，狗，猫等动物，但是文章中没有动物两个字，so，前面两种算法就不能提取出动物这个隐含的主题信息。</p>
<p>###LSA（LSI）<br>LSA(Latent Semantic Analysis,潜在语义分析）LSI（Latent Semantic index,潜在语义索引）</p>
<p>LSA算法步骤：</p>
<p>1）使用BOW模型将每个文档表示为向量</p>
<p>2）将文档词向量拼接起来构成词-文档矩阵（mxn）；</p>
<p>3） 使用奇异值分解（SVD） 操作（[mxr].[rxr].[r*n])</p>
<p>4) 将文档映射到更低维度k（[mxk].[kxk] .[k*n])</p>
<p>每个词和文档都可以表示为k个主题构成的空间中的一个点，通过计算每个词和文档的相似度（余弦相似度orKl相似度），得到每个文档中的每个词的相似度结果。取相似度最高的词即为关键词。</p>
<p>SVD的计算复杂度非常高，特征空间维度较大，计算效率十分低。</p>
<p>###LDA算法</p>
<p>1 对d维数据进行标准化处理（d为特征数量）</p>
<p>2 对于每一类别，计算d维的均值向量</p>
<p>3 构造类间的散布矩阵 SBSB 以及 类内散布矩阵 SWSW</p>
<p>4 计算矩阵 S−1WSBSW−1SB 的特征值以及对应的特征向量</p>
<p>5 选取前k个特征值所对应的特征向量，构造一个 d∗kd∗k 维的转换矩阵 WW,其中特征向量以列的形式排列</p>
<p>6 使用转换矩阵 WW 将样本映射到新的特征子空间上</p>
<p><strong>代码：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">author: heucoder</span></span><br><span class="line"><span class="string">email: 812860165@qq.com</span></span><br><span class="line"><span class="string">date: 2019.6.13</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lda</span><span class="params">(data, target, n_dim)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    :param data: (n_samples, n_features)</span></span><br><span class="line"><span class="string">    :param target: data class</span></span><br><span class="line"><span class="string">    :param n_dim: target dimension</span></span><br><span class="line"><span class="string">    :return: (n_samples, n_dims)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    clusters = np.unique(target)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> n_dim &gt; len(clusters) - <span class="number">1</span>:</span><br><span class="line">        print(<span class="string">"K is too much"</span>)</span><br><span class="line">        print(<span class="string">"please input again"</span>)</span><br><span class="line">        exit(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># within_class scatter matrix</span></span><br><span class="line">    Sw = np.zeros((data.shape[<span class="number">1</span>], data.shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> clusters:</span><br><span class="line">        datai = data[target == i]</span><br><span class="line">        datai = datai - datai.mean(<span class="number">0</span>)</span><br><span class="line">        Swi = np.mat(datai).T * np.mat(datai)</span><br><span class="line">        Sw += Swi</span><br><span class="line"></span><br><span class="line">    <span class="comment"># between_class scatter matrix</span></span><br><span class="line">    SB = np.zeros((data.shape[<span class="number">1</span>], data.shape[<span class="number">1</span>]))</span><br><span class="line">    u = data.mean(<span class="number">0</span>)  <span class="comment"># 所有样本的平均值</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> clusters:</span><br><span class="line">        Ni = data[target == i].shape[<span class="number">0</span>]</span><br><span class="line">        ui = data[target == i].mean(<span class="number">0</span>)  <span class="comment"># 某个类别的平均值</span></span><br><span class="line">        SBi = Ni * np.mat(ui - u).T * np.mat(ui - u)</span><br><span class="line">        SB += SBi</span><br><span class="line">    S = np.linalg.inv(Sw) * SB</span><br><span class="line">    eigVals, eigVects = np.linalg.eig(S)  <span class="comment"># 求特征值，特征向量</span></span><br><span class="line">    eigValInd = np.argsort(eigVals)</span><br><span class="line">    eigValInd = eigValInd[:(-n_dim - <span class="number">1</span>):<span class="number">-1</span>]</span><br><span class="line">    w = eigVects[:, eigValInd]</span><br><span class="line">    data_ndim = np.dot(data, w)</span><br><span class="line">    print(data_ndim)</span><br><span class="line">    <span class="keyword">return</span> data_ndim</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    iris = load_iris()</span><br><span class="line">    X = iris.data</span><br><span class="line">    Y = iris.target</span><br><span class="line">    data_1 = lda(X, Y, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    data_2 = LinearDiscriminantAnalysis(n_components=<span class="number">2</span>).fit_transform(X, Y)</span><br><span class="line"></span><br><span class="line">    plt.figure(figsize=(<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line">    plt.subplot(<span class="number">121</span>)</span><br><span class="line">    plt.title(<span class="string">"my_LDA"</span>)</span><br><span class="line">    plt.scatter(data_1[:, <span class="number">0</span>], data_1[:, <span class="number">1</span>], c=Y)</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">122</span>)</span><br><span class="line">    plt.title(<span class="string">"sklearn_LDA"</span>)</span><br><span class="line">    plt.scatter(data_2[:, <span class="number">0</span>], data_2[:, <span class="number">1</span>], c=Y)</span><br><span class="line">    plt.savefig(<span class="string">"LDA.png"</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/12/01/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96%E7%AE%97%E6%B3%95/" data-id="ck3o9akr10004ucvjamj79jru" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-KPCA线性降维算法 " class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/12/01/KPCA%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%20/" class="article-date">
  <time datetime="2019-12-01T05:41:39.418Z" itemprop="datePublished">2019-12-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/01/KPCA%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%20/">KPCA</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="特征抽取"><a href="#特征抽取" class="headerlink" title="特征抽取"></a>特征抽取</h2><p>可以将原始数据集变换到一个维度更低的新的特征子空间，在尽可能多地保持相关信息的情况下，对数据进行压缩</p>
<p>##线性可分 VS 非线性可分</p>
<p><img src="https://github.com/rasbt/python-machine-learning-book/raw/master/code/ch05/images/05_11.png" alt="fsaf"></p>
<p>可以通过kPCA将非线性数据映射到高维空间，在高维空间下使用标准PCA将其映射到另一个低维空间</p>
<p><img src="https://github.com/KARL13YAN/my-blogs/raw/master/pythonML/kPCA.png" alt=""></p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>定义非线性映射函数，该函数可以对原始特征进行非线性组合，以将原始的d维数据集映射到更高维的k维特征空间。</p>
<p>##使用Python实现kPCA<br>    from scipy.spatial.distance import pdist, squareform<br>    from scipy import exp<br>    from numpy.linalg import eigh<br>    import numpy as np</p>
<pre><code>def rbf_kernel_pca(X, gamma, n_components):
    &quot;&quot;&quot;
    RBF kernel PCA implementation.

    Parameters
    ------------
    X: {NumPy ndarray}, shape = [n_samples, n_features]

    gamma: float
      Tuning parameter of the RBF kernel

    n_components: int
      Number of principal components to return

    Returns
    ------------
     X_pc: {NumPy ndarray}, shape = [n_samples, k_features]
       Projected dataset   

    &quot;&quot;&quot;
    # Calculate pairwise squared Euclidean distances
    # in the MxN dimensional dataset.
    sq_dists = pdist(X, &apos;sqeuclidean&apos;)

    # Convert pairwise distances into a square matrix.
    mat_sq_dists = squareform(sq_dists)

    # Compute the symmetric kernel matrix.
    K = exp(-gamma * mat_sq_dists)

    # Center the kernel matrix.
    N = K.shape[0]
    one_n = np.ones((N, N)) / N
    K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)

    # Obtaining eigenpairs from the centered kernel matrix
    # numpy.linalg.eigh returns them in sorted order
    eigvals, eigvecs = eigh(K)

    # Collect the top k eigenvectors (projected samples)
    X_pc = np.column_stack((eigvecs[:, -i]
                            for i in range(1, n_components + 1)))

    return X_pc</code></pre><h2 id="创建如下数据"><a href="#创建如下数据" class="headerlink" title="创建如下数据"></a>创建如下数据</h2><pre><code>import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
X, y = make_moons(n_samples=100, random_state=123)
plt.scatter(X[y == 0, 0], X[y == 0, 1], color=&apos;red&apos;, marker=&apos;^&apos;, alpha=0.5)
plt.scatter(X[y == 1, 0], X[y == 1, 1], color=&apos;blue&apos;, marker=&apos;o&apos;, alpha=0.5)

plt.tight_layout()
plt.show()</code></pre><h2 id="使用PCA算法"><a href="#使用PCA算法" class="headerlink" title="使用PCA算法"></a>使用PCA算法</h2><pre><code>from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

scikit_pca = PCA(n_components=2)
X_spca = scikit_pca.fit_transform(X)

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))

ax[0].scatter(X_spca[y == 0, 0], X_spca[y == 0, 1],
              color=&apos;red&apos;, marker=&apos;^&apos;, alpha=0.5)
ax[0].scatter(X_spca[y == 1, 0], X_spca[y == 1, 1],
              color=&apos;blue&apos;, marker=&apos;o&apos;, alpha=0.5)

ax[1].scatter(X_spca[y == 0, 0], np.zeros((50, 1)) + 0.02,
              color=&apos;red&apos;, marker=&apos;^&apos;, alpha=0.5)
ax[1].scatter(X_spca[y == 1, 0], np.zeros((50, 1)) - 0.02,
              color=&apos;blue&apos;, marker=&apos;o&apos;, alpha=0.5)

ax[0].set_xlabel(&apos;PC1&apos;)
ax[0].set_ylabel(&apos;PC2&apos;)
ax[1].set_ylim([-1, 1])
ax[1].set_yticks([])
ax[1].set_xlabel(&apos;PC1&apos;)

plt.tight_layout()
# plt.savefig(&apos;./figures/half_moon_2.png&apos;, dpi=300)
plt.show()</code></pre><h2 id="使用KPCA算法"><a href="#使用KPCA算法" class="headerlink" title="使用KPCA算法"></a>使用KPCA算法</h2><pre><code>from matplotlib.ticker import FormatStrFormatter

X_kpca = rbf_kernel_pca(X, gamma=15, n_components=2)

fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(7,3))
ax[0].scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], 
            color=&apos;red&apos;, marker=&apos;^&apos;, alpha=0.5)
ax[0].scatter(X_kpca[y==1, 0], X_kpca[y==1, 1],
            color=&apos;blue&apos;, marker=&apos;o&apos;, alpha=0.5)

ax[1].scatter(X_kpca[y==0, 0], np.zeros((50,1))+0.02, 
            color=&apos;red&apos;, marker=&apos;^&apos;, alpha=0.5)
ax[1].scatter(X_kpca[y==1, 0], np.zeros((50,1))-0.02,
            color=&apos;blue&apos;, marker=&apos;o&apos;, alpha=0.5)

ax[0].set_xlabel(&apos;PC1&apos;)
ax[0].set_ylabel(&apos;PC2&apos;)
ax[1].set_ylim([-1, 1])
ax[1].set_yticks([])
ax[1].set_xlabel(&apos;PC1&apos;)
ax[0].xaxis.set_major_formatter(FormatStrFormatter(&apos;%0.1f&apos;))
ax[1].xaxis.set_major_formatter(FormatStrFormatter(&apos;%0.1f&apos;))

plt.tight_layout()
# plt.savefig(&apos;./figures/half_moon_3.png&apos;, dpi=300)
plt.show()</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/12/01/KPCA%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%20/" data-id="ck3o9akqu0001ucvjg8562yag" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-PCA线性降维算法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/12/01/PCA%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95/" class="article-date">
  <time datetime="2019-12-01T04:51:06.192Z" itemprop="datePublished">2019-12-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/01/PCA%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95/">PCA</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="协方差Cov（x，y）"><a href="#协方差Cov（x，y）" class="headerlink" title="协方差Cov（x，y）"></a>协方差Cov（x，y）</h2><p>COV(X,Y)=E(XY)-E(X)E(Y)<br>COV(X,Y)=[D(X)+D(Y)-D(X+Y)]/2</p>
<h2 id="PCA概念"><a href="#PCA概念" class="headerlink" title="PCA概念"></a>PCA概念</h2><p>PCA 是一种基于从高维空间映射到低维空间的映射方法，也是最基础的无监督降维算法，其目标是向数据变化最大的方向投影，或者说向重构误差最小化的方向投影。它由 Karl Pearson 在 1901 年提出，属于线性降维方法。与 PCA 相关的原理通常被称为最大方差理论或最小误差理论。这两者目标一致，但过程侧重点则不同。</p>
<h2 id="最大方法降维原理"><a href="#最大方法降维原理" class="headerlink" title="最大方法降维原理"></a>最大方法降维原理</h2><p>将一组 N 维向量降为 K 维（K 大于 0，小于 N），其目标是选择 K 个单位正交基，各字段两两间 COV(X,Y) 为 0，而字段的方差则尽可能大。因此，最大方差即使得投影数据的方差被最大化，在这过程中，我们需要找到数据集 Xmxn 的最佳的投影空间 Wnxk、协方差矩阵等，其算法流程为<br>算法输入：数据集 Xmxn；</p>
<p>按列计算数据集 X 的均值 Xmean，然后令 Xnew=XXmean；</p>
<p>求解矩阵 Xnew 的协方差矩阵，并将其记为 Cov；</p>
<p>计算协方差矩阵 COv 的特征值和相应的特征向量；</p>
<p>将特征值按照从大到小的排序，选择其中最大的 k 个，然后将其对应的 k 个特征向量分别作为列向量组成特征向量矩阵 Wnxk；</p>
<p>计算 XnewW，即将数据集 Xnew 投影到选取的特征向量上，这样就得到了我们需要的已经降维的数据集 XnewW。</p>
<h2 id="PCA算法优缺点"><a href="#PCA算法优缺点" class="headerlink" title="PCA算法优缺点"></a>PCA算法优缺点</h2><p>优点：</p>
<p>它是无监督学习，完全无参数限制的。在PCA的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。</p>
<p>用PCA技术可以对数据进行降维，同时对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。</p>
<p>各主成分之间正交，可消除原始数据成分间的相互影响。</p>
<p>计算方法简单，易于在计算机上实现。</p>
<p>缺点：</p>
<p>如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。</p>
<p>贡献率小的主成分往往可能含有对样本差异的重要信息。</p>
<p>特征值矩阵的正交向量空间是否唯一有待讨论。</p>
<p>在非高斯分布的情况下，PCA方法得出的主元可能并不是最优的，此时在寻找主元时不能将方差作为衡量重要性的标准。</p>
<h2 id="python算法实现"><a href="#python算法实现" class="headerlink" title="python算法实现"></a>python算法实现</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.cm <span class="keyword">as</span> cmx</span><br><span class="line"><span class="keyword">import</span> matplotlib.colors <span class="keyword">as</span> colors</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shuffle_data</span><span class="params">(X, y, seed=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> seed:</span><br><span class="line">        np.random.seed(seed)</span><br><span class="line"></span><br><span class="line">    idx = np.arange(X.shape[<span class="number">0</span>])</span><br><span class="line">    np.random.shuffle(idx)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X[idx], y[idx]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 正规化数据集 X</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span><span class="params">(X, axis=<span class="number">-1</span>, p=<span class="number">2</span>)</span>:</span></span><br><span class="line">    lp_norm = np.atleast_1d(np.linalg.norm(X, p, axis))</span><br><span class="line">    lp_norm[lp_norm == <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> X / np.expand_dims(lp_norm, axis)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化数据集 X</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standardize</span><span class="params">(X)</span>:</span></span><br><span class="line">    X_std = np.zeros(X.shape)</span><br><span class="line">    mean = X.mean(axis=<span class="number">0</span>)</span><br><span class="line">    std = X.std(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 做除法运算时请永远记住分母不能等于0的情形</span></span><br><span class="line">    <span class="comment"># X_std = (X - X.mean(axis=0)) / X.std(axis=0) </span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> range(np.shape(X)[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> std[col]:</span><br><span class="line">            X_std[:, col] = (X_std[:, col] - mean[col]) / std[col]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_std</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分数据集为训练集和测试集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_test_split</span><span class="params">(X, y, test_size=<span class="number">0.2</span>, shuffle=True, seed=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        X, y = shuffle_data(X, y, seed)</span><br><span class="line"></span><br><span class="line">    n_train_samples = int(X.shape[<span class="number">0</span>] * (<span class="number">1</span>-test_size))</span><br><span class="line">    x_train, x_test = X[:n_train_samples], X[n_train_samples:]</span><br><span class="line">    y_train, y_test = y[:n_train_samples], y[n_train_samples:]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_train, x_test, y_train, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算矩阵X的协方差矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_covariance_matrix</span><span class="params">(X, Y=np.empty<span class="params">(<span class="params">(<span class="number">0</span>,<span class="number">0</span>)</span>)</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> Y.any():</span><br><span class="line">        Y = X</span><br><span class="line">    n_samples = np.shape(X)[<span class="number">0</span>]</span><br><span class="line">    covariance_matrix = (<span class="number">1</span> / (n_samples<span class="number">-1</span>)) * (X - X.mean(axis=<span class="number">0</span>)).T.dot(Y - Y.mean(axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.array(covariance_matrix, dtype=float)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算数据集X每列的方差</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_variance</span><span class="params">(X)</span>:</span></span><br><span class="line">    n_samples = np.shape(X)[<span class="number">0</span>]</span><br><span class="line">    variance = (<span class="number">1</span> / n_samples) * np.diag((X - X.mean(axis=<span class="number">0</span>)).T.dot(X - X.mean(axis=<span class="number">0</span>)))</span><br><span class="line">    <span class="keyword">return</span> variance</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算数据集X每列的标准差</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_std_dev</span><span class="params">(X)</span>:</span></span><br><span class="line">    std_dev = np.sqrt(calculate_variance(X))</span><br><span class="line">    <span class="keyword">return</span> std_dev</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算相关系数矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_correlation_matrix</span><span class="params">(X, Y=np.empty<span class="params">([<span class="number">0</span>])</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 先计算协方差矩阵</span></span><br><span class="line">    covariance_matrix = calculate_covariance_matrix(X, Y)</span><br><span class="line">    <span class="comment"># 计算X, Y的标准差</span></span><br><span class="line">    std_dev_X = np.expand_dims(calculate_std_dev(X), <span class="number">1</span>)</span><br><span class="line">    std_dev_y = np.expand_dims(calculate_std_dev(Y), <span class="number">1</span>)</span><br><span class="line">    correlation_matrix = np.divide(covariance_matrix, std_dev_X.dot(std_dev_y.T))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.array(correlation_matrix, dtype=float)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PCA</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    主成份分析算法PCA，非监督学习算法.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.eigen_values = <span class="literal">None</span></span><br><span class="line">        self.eigen_vectors = <span class="literal">None</span></span><br><span class="line">        self.k = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" </span></span><br><span class="line"><span class="string">        将原始数据集X通过PCA进行降维</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        covariance = calculate_covariance_matrix(X)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 求解特征值和特征向量</span></span><br><span class="line">        self.eigen_values, self.eigen_vectors = np.linalg.eig(covariance)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将特征值从大到小进行排序，注意特征向量是按列排的，即self.eigen_vectors第k列是self.eigen_values中第k个特征值对应的特征向量</span></span><br><span class="line">        idx = self.eigen_values.argsort()[::<span class="number">-1</span>]</span><br><span class="line">        eigenvalues = self.eigen_values[idx][:self.k]</span><br><span class="line">        eigenvectors = self.eigen_vectors[:, idx][:, :self.k]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将原始数据集X映射到低维空间</span></span><br><span class="line">        X_transformed = X.dot(eigenvectors)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X_transformed</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># Load the dataset</span></span><br><span class="line">    data = datasets.load_iris()</span><br><span class="line">    X = data.data</span><br><span class="line">    y = data.target</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将数据集X映射到低维空间</span></span><br><span class="line">    X_trans = PCA().transform(X)</span><br><span class="line"></span><br><span class="line">    x1 = X_trans[:, <span class="number">0</span>]</span><br><span class="line">    x2 = X_trans[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    cmap = plt.get_cmap(<span class="string">'viridis'</span>)</span><br><span class="line">    colors = [cmap(i) <span class="keyword">for</span> i <span class="keyword">in</span> np.linspace(<span class="number">0</span>, <span class="number">1</span>, len(np.unique(y)))]</span><br><span class="line"></span><br><span class="line">    class_distr = []</span><br><span class="line">    <span class="comment"># Plot the different class distributions</span></span><br><span class="line">    <span class="keyword">for</span> i, l <span class="keyword">in</span> enumerate(np.unique(y)):</span><br><span class="line">        _x1 = x1[y == l]</span><br><span class="line">        _x2 = x2[y == l]</span><br><span class="line">        _y = y[y == l]</span><br><span class="line">        class_distr.append(plt.scatter(_x1, _x2, color=colors[i]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add a legend</span></span><br><span class="line">    plt.legend(class_distr, y, loc=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Axis labels</span></span><br><span class="line">    plt.xlabel(<span class="string">'Principal Component 1'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Principal Component 2'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>


<p>```</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/12/01/PCA%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95/" data-id="ck3o9akqy0002ucvj1joa7xzq" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-new-Post" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/11/07/new-Post/" class="article-date">
  <time datetime="2019-11-07T10:58:49.000Z" itemprop="datePublished">2019-11-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/11/07/new-Post/">new Post</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>我要这个天下</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/11/07/new-Post/" data-id="ck3o9akqz0003ucvj1jn247fl" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%87%AA%E8%BF%B0/" rel="tag">自述</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-My-Gallery" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/11/07/My-Gallery/" class="article-date">
  <time datetime="2019-11-07T09:40:36.306Z" itemprop="datePublished">2019-11-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/11/07/My-Gallery/">My Gallery</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Part-1-Count-Based-Word-Vectors"><a href="#Part-1-Count-Based-Word-Vectors" class="headerlink" title="Part 1: Count-Based Word Vectors"></a>Part 1: Count-Based Word Vectors</h1><p>Most word vector models start from the following idea:</p>
<p><em>You shall know a word by the company it keeps (<a href="https://en.wikipedia.org/wiki/John_Rupert_Firth" target="_blank" rel="noopener">Firth, J. R. 1957:11</a>)</em></p>
<p>Many word vector implementations are driven by the idea that similar words, i.e., (near) synonyms, will be used in similar contexts. As a result, similar words will often be spoken or written along with a shared subset of words, i.e., contexts. By examining these contexts, we can try to develop embeddings for our words. With this intuition in mind, many “old school” approaches to constructing word vectors relied on word counts. Here we elaborate upon one of those strategies, <em>co-occurrence matrices</em> (for more information, see <a href="http://web.stanford.edu/class/cs124/lec/vectorsemantics.video.pdf" target="_blank" rel="noopener">here</a> or <a href="https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285" target="_blank" rel="noopener">here</a>).</p>
<h2 id="Word-Vectors"><a href="#Word-Vectors" class="headerlink" title="Word Vectors"></a>Word Vectors</h2><p>Word Vectors are often used as a fundamental component for downstream NLP tasks, e.g. question answering, text generation, translation, etc., so it is important to build some intuitions as to their strengths and weaknesses. Here, you will explore two types of word vectors: those derived from <em>co-occurrence matrices</em>, and those derived via <em>word2vec</em>. </p>
<p><strong>Assignment Notes:</strong> Please make sure to save the notebook as you go along. Submission Instructions are located at the bottom of the notebook.</p>
<p><strong>Note on Terminology:</strong> The terms “word vectors” and “word embeddings” are often used interchangeably. The term “embedding” refers to the fact that we are encoding aspects of a word’s meaning in a lower dimensional space. As <a href="https://en.wikipedia.org/wiki/Word_embedding" target="_blank" rel="noopener">Wikipedia</a> states, “<em>conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension</em>“.</p>
<h1 id="Co-Occurrence"><a href="#Co-Occurrence" class="headerlink" title="Co-Occurrence"></a>Co-Occurrence</h1><p>A co-occurrence matrix counts how often things co-occur in some environment. Given some word $w_i$ occurring in the document, we consider the <em>context window</em> surrounding $w_i$. Supposing our fixed window size is $n$, then this is the $n$ preceding and $n$ subsequent words in that document, i.e. words $w_{i-n} \dots w_{i-1}$ and $w_{i+1} \dots w_{i+n}$. We build a <em>co-occurrence matrix</em> $M$, which is a symmetric word-by-word matrix in which $M_{ij}$ is the number of times $w_j$ appears inside $w_i$’s window.</p>
<p><strong>Example: Co-Occurrence with Fixed Window of n=1</strong>:</p>
<p>Document 1: “all that glitters is not gold”</p>
<p>Document 2: “all is well that ends well”</p>
<table>
<thead>
<tr>
<th>*</th>
<th>START</th>
<th>all</th>
<th>that</th>
<th>glitters</th>
<th>is</th>
<th>not</th>
<th>gold</th>
<th>well</th>
<th>ends</th>
<th>END</th>
</tr>
</thead>
<tbody><tr>
<td>START</td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>all</td>
<td>2</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>that</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>glitters</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>is</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>not</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>gold</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>well</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>ends</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>END</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody></table>
<p><strong>Note:</strong> In NLP, we often add START and END tokens to represent the beginning and end of sentences, paragraphs or documents. In thise case we imagine START and END tokens encapsulating each document, e.g., “START All that glitters is not gold END”, and include these tokens in our co-occurrence counts.</p>
<p>The rows (or columns) of this matrix provide one type of word vectors (those based on word-word co-occurrence), but the vectors will be large in general (linear in the number of distinct words in a corpus). Thus, our next step is to run <em>dimensionality reduction</em>. In particular, we will run <em>SVD (Singular Value Decomposition)</em>, which is a kind of generalized <em>PCA (Principal Components Analysis)</em> to select the top $k$ principal components. Here’s a visualization of dimensionality reduction with SVD. In this picture our co-occurrence matrix is $A$ with $n$ rows corresponding to $n$ words. We obtain a full matrix decomposition, with the singular values ordered in the diagonal $S$ matrix, and our new, shorter length-$k$ word vectors in $U_k$.</p>
<p><img src="imgs/svd.png" alt="Picture of an SVD" title="SVD"></p>
<p>This reduced-dimensionality co-occurrence representation preserves semantic relationships between words, e.g. <em>doctor</em> and <em>hospital</em> will be closer than <em>doctor</em> and <em>dog</em>. </p>
<p><strong>Notes:</strong> If you can barely remember what an eigenvalue is, here’s <a href="https://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf" target="_blank" rel="noopener">a slow, friendly introduction to SVD</a>. If you want to learn more thoroughly about PCA or SVD, feel free to check out lectures <a href="https://web.stanford.edu/class/cs168/l/l7.pdf" target="_blank" rel="noopener">7</a>, <a href="http://theory.stanford.edu/~tim/s15/l/l8.pdf" target="_blank" rel="noopener">8</a>, and <a href="https://web.stanford.edu/class/cs168/l/l9.pdf" target="_blank" rel="noopener">9</a> of CS168. These course notes provide a great high-level treatment of these general purpose algorithms. Though, for the purpose of this class, you only need to know how to extract the k-dimensional embeddings by utilizing pre-programmed implementations of these algorithms from the numpy, scipy, or sklearn python packages. In practice, it is challenging to apply full SVD to large corpora because of the memory needed to perform PCA or SVD. However, if you only want the top $k$ vector components for relatively small $k$ — known as <em><a href="https://en.wikipedia.org/wiki/Singular_value_decomposition#Truncated_SVD" target="_blank" rel="noopener">Truncated SVD</a></em> — then there are reasonably scalable techniques to compute those iteratively.</p>
<h3 id="Plotting-Co-Occurrence-Word-Embeddings"><a href="#Plotting-Co-Occurrence-Word-Embeddings" class="headerlink" title="Plotting Co-Occurrence Word Embeddings"></a>Plotting Co-Occurrence Word Embeddings</h3><p>Here, we will be using the Reuters (business and financial news) corpus. If you haven’t run the import cell at the top of this page, please run it now (click it and press SHIFT-RETURN). The corpus consists of 10,788 news documents totaling 1.3 million words. These documents span 90 categories and are split into train and test. For more details, please see <a href="https://www.nltk.org/book/ch02.html" target="_blank" rel="noopener">https://www.nltk.org/book/ch02.html</a>. We provide a <code>read_corpus</code> function below that pulls out only articles from the “crude” (i.e. news articles about oil, gas, etc.) category. The function also adds START and END tokens to each of the documents, and lowercases words. You do <strong>not</strong> have perform any other kind of pre-processing.</p>
<h3 id="Question-1-1-Implement-distinct-words-code-2-points"><a href="#Question-1-1-Implement-distinct-words-code-2-points" class="headerlink" title="Question 1.1: Implement distinct_words [code] (2 points)"></a>Question 1.1: Implement <code>distinct_words</code> [code] (2 points)</h3><p>Write a method to work out the distinct words (word types) that occur in the corpus. You can do this with <code>for</code> loops, but it’s more efficient to do it with Python list comprehensions. In particular, <a href="https://coderwall.com/p/rcmaea/flatten-a-list-of-lists-in-one-line-in-python" target="_blank" rel="noopener">this</a> may be useful to flatten a list of lists. If you’re not familiar with Python list comprehensions in general, here’s <a href="https://python-3-patterns-idioms-test.readthedocs.io/en/latest/Comprehensions.html" target="_blank" rel="noopener">more information</a>.</p>
<p>You may find it useful to use <a href="https://www.w3schools.com/python/python_sets.asp" target="_blank" rel="noopener">Python sets</a> to remove duplicate words.</p>
<h3 id="Question-1-2-Implement-compute-co-occurrence-matrix-code-3-points"><a href="#Question-1-2-Implement-compute-co-occurrence-matrix-code-3-points" class="headerlink" title="Question 1.2: Implement compute_co_occurrence_matrix [code] (3 points)"></a>Question 1.2: Implement <code>compute_co_occurrence_matrix</code> [code] (3 points)</h3><p>Write a method that constructs a co-occurrence matrix for a certain window-size $n$ (with a default of 4), considering words $n$ before and $n$ after the word in the center of the window. Here, we start to use <code>numpy (np)</code> to represent vectors, matrices, and tensors. If you’re not familiar with NumPy, there’s a NumPy tutorial in the second half of this cs231n <a href="http://cs231n.github.io/python-numpy-tutorial/" target="_blank" rel="noopener">Python NumPy tutorial</a>.</p>
<h3 id="Question-1-3-Implement-reduce-to-k-dim-code-1-point"><a href="#Question-1-3-Implement-reduce-to-k-dim-code-1-point" class="headerlink" title="Question 1.3: Implement reduce_to_k_dim [code] (1 point)"></a>Question 1.3: Implement <code>reduce_to_k_dim</code> [code] (1 point)</h3><p>Construct a method that performs dimensionality reduction on the matrix to produce k-dimensional embeddings. Use SVD to take the top k components and produce a new matrix of k-dimensional embeddings. </p>
<p><strong>Note:</strong> All of numpy, scipy, and scikit-learn (<code>sklearn</code>) provide <em>some</em> implementation of SVD, but only scipy and sklearn provide an implementation of Truncated SVD, and only sklearn provides an efficient randomized algorithm for calculating large-scale Truncated SVD. So please use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html" target="_blank" rel="noopener">sklearn.decomposition.TruncatedSVD</a>.</p>
<h3 id="Question-1-4-Implement-plot-embeddings-code-1-point"><a href="#Question-1-4-Implement-plot-embeddings-code-1-point" class="headerlink" title="Question 1.4: Implement plot_embeddings [code] (1 point)"></a>Question 1.4: Implement <code>plot_embeddings</code> [code] (1 point)</h3><p>Here you will write a function to plot a set of 2D vectors in 2D space. For graphs, we will use Matplotlib (<code>plt</code>).</p>
<p>For this example, you may find it useful to adapt <a href="https://www.pythonmembers.club/2018/05/08/matplotlib-scatter-plot-annotate-set-text-at-label-each-point/" target="_blank" rel="noopener">this code</a>. In the future, a good way to make a plot is to look at <a href="https://matplotlib.org/gallery/index.html" target="_blank" rel="noopener">the Matplotlib gallery</a>, find a plot that looks somewhat like what you want, and adapt the code they give.</p>
<h3 id="Question-1-5-Co-Occurrence-Plot-Analysis-written-3-points"><a href="#Question-1-5-Co-Occurrence-Plot-Analysis-written-3-points" class="headerlink" title="Question 1.5: Co-Occurrence Plot Analysis [written] (3 points)"></a>Question 1.5: Co-Occurrence Plot Analysis [written] (3 points)</h3><p>Now we will put together all the parts you have written! We will compute the co-occurrence matrix with fixed window of 4, over the Reuters “crude” corpus. Then we will use TruncatedSVD to compute 2-dimensional embeddings of each word. TruncatedSVD returns U*S, so we normalize the returned vectors, so that all the vectors will appear around the unit circle (therefore closeness is directional closeness). <strong>Note</strong>: The line of code below that does the normalizing uses the NumPy concept of <em>broadcasting</em>. If you don’t know about broadcasting, check out<br><a href="https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html" target="_blank" rel="noopener">Computation on Arrays: Broadcasting by Jake VanderPlas</a>.</p>
<p>Run the below cell to produce the plot. It’ll probably take a few seconds to run. What clusters together in 2-dimensional embedding space? What doesn’t cluster together that you might think should have?  <strong>Note:</strong> “bpd” stands for “barrels per day” and is a commonly used abbreviation in crude oil topic articles.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/11/07/My-Gallery/" data-id="ck3o9akrn0007ucvja97sgmkh" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/11/04/hello-world/" class="article-date">
  <time datetime="2019-11-04T13:04:40.999Z" itemprop="datePublished">2019-11-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/11/04/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/11/04/hello-world/" data-id="ck3o9akqk0000ucvj4dylhdug" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%87%AA%E8%BF%B0/" rel="tag">自述</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/%E8%87%AA%E8%BF%B0/" style="font-size: 10px;">自述</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/12/01/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96%E7%AE%97%E6%B3%95/">关键词提取</a>
          </li>
        
          <li>
            <a href="/2019/12/01/KPCA%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%20/">KPCA</a>
          </li>
        
          <li>
            <a href="/2019/12/01/PCA%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95/">PCA</a>
          </li>
        
          <li>
            <a href="/2019/11/07/new-Post/">new Post</a>
          </li>
        
          <li>
            <a href="/2019/11/07/My-Gallery/">My Gallery</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>