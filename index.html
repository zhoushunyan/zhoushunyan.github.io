<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-python学习/hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/06/python%E5%AD%A6%E4%B9%A0/hello-world/" class="article-date">
  <time datetime="2020-01-06T05:11:49.099Z" itemprop="datePublished">2020-01-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/06/python%E5%AD%A6%E4%B9%A0/hello-world/">高阶函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="慢慢来"><a href="#慢慢来" class="headerlink" title="慢慢来"></a>慢慢来</h2><p>###More info: <a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017328655674400" target="_blank" rel="noopener">Writing</a></p>
<p>###第一步</p>
<p>变量可以指向函数</p>
<pre><code>&gt;&gt;&gt; abs(-10)
10
&gt;&gt;&gt; abs
&lt;built-in function abs&gt;
&gt;&gt;&gt; x = abs(-10)
&gt;&gt;&gt; x
10</code></pre><p>但是，如果把函数本身赋值给变量呢？结论：函数本身也可以赋值给变量，即：变量可以指向函数。</p>
<p>如果一个变量指向了一个函数，那么，可否通过该变量来调用这个函数？用代码验证一下：</p>
<pre><code>&gt;&gt;&gt; f = abs
&gt;&gt;&gt; f
&lt;built-in function abs&gt;
&gt;&gt;&gt; f(-10)
10</code></pre><p>成功！说明变量f现在已经指向了abs函数本身。直接调用abs()函数和调用变量f()完全相同。</p>
<p>那么函数名是什么呢？函数名其实就是指向函数的变量！对于abs()这个函数，完全可以把函数名abs看成变量，它指向一个可以计算绝对值的函数！</p>
<p>如果把abs指向其他对象，会有什么情况发生？</p>
<pre><code>&gt;&gt;&gt; abs = 10
&gt;&gt;&gt; abs(-10)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
TypeError: &apos;int&apos; object is not callable</code></pre><p>###高阶函数：既然变量可以指向函数，函数的参数能接收变量，那么一个函数就可以接收另一个函数作为参数，这种函数就称之为高阶函数。</p>
<p>一个最简单的高阶函数：</p>
<pre><code>def add(x, y, f):
    return f(x) + f(y)
推导过程

x = -5
y = 6
f = abs
f(x) + f(y) ==&gt; abs(-5) + abs(6) ==&gt; 11
return 11</code></pre><p>##map/reduce</p>
<p>我们先看map。map()函数接收两个参数，一个是函数，一个是Iterable，map将传入的函数依次作用到序列的每个元素，并把结果作为新的Iterator返回。</p>
<p>举例说明，比如我们有一个函数f(x)=x2，要把这个函数作用在一个list [1, 2, 3, 4, 5, 6, 7, 8, 9]上，就可以用map()实现如下：</p>
<pre><code>&gt;&gt;&gt; def f(x):
...     return x * x
...
map使用
&gt;&gt;&gt; r = map(f, [1, 2, 3, 4, 5, 6, 7, 8, 9])
&gt;&gt;&gt; list(r)
[1, 4, 9, 16, 25, 36, 49, 64, 81]
循环使用
L = []
for n in [1, 2, 3, 4, 5, 6, 7, 8, 9]:
    L.append(f(n))
print(L)</code></pre><p>###所以，map()作为高阶函数，事实上它把运算规则抽象了，因此，我们不但可以计算简单的f(x)=x2，还可以计算任意复杂的函数，比如，把这个list所有数字转为字符串</p>
<pre><code>&gt;&gt;&gt; list(map(str, [1, 2, 3, 4, 5, 6, 7, 8, 9]))
[&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;4&apos;, &apos;5&apos;, &apos;6&apos;, &apos;7&apos;, &apos;8&apos;, &apos;9&apos;]</code></pre><p>###再看reduce的用法。reduce把一个函数作用在一个序列[x1, x2, x3, …]上，这个函数必须接收两个参数，reduce把结果继续和序列的下一个元素做累积计算，其效果就是：</p>
<pre><code>reduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4)</code></pre><p>比方说对一个序列求和，就可以用reduce实现：</p>
<pre><code>&gt;&gt;&gt; from functools import reduce
&gt;&gt;&gt; def add(x, y):
...     return x + y
...
&gt;&gt;&gt; reduce(add, [1, 3, 5, 7, 9])
25
使用sum（）函数
sum([1, 3, 5, 7, 9])</code></pre><p>但是如果要把序列[1, 3, 5, 7, 9]变换成整数13579，reduce就可以派上用场：</p>
<pre><code>&gt;&gt;&gt; from functools import reduce
&gt;&gt;&gt; def fn(x, y):
...     return x * 10 + y
...
&gt;&gt;&gt; reduce(fn, [1, 3, 5, 7, 9])
13579</code></pre><p>这个例子本身没多大用处，但是，如果考虑到字符串str也是一个序列，对上面的例子稍加改动，配合map()，我们就可以写出把str转换为int的函数</p>
<pre><code>&gt;&gt;&gt; from functools import reduce
&gt;&gt;&gt; def fn(x, y):
...     return x * 10 + y
...
&gt;&gt;&gt; def char2num(s):
...     digits = {&apos;0&apos;: 0, &apos;1&apos;: 1, &apos;2&apos;: 2, &apos;3&apos;: 3, &apos;4&apos;: 4, &apos;5&apos;: 5, &apos;6&apos;: 6, &apos;7&apos;: 7, &apos;8&apos;: 8, &apos;9&apos;: 9}
...     return digits[s]
...
&gt;&gt;&gt; reduce(fn, map(char2num, &apos;13579&apos;))
13579</code></pre><p>整理成一个str2int的函数就是：</p>
<pre><code>from functools import reduce

DIGITS = {&apos;0&apos;: 0, &apos;1&apos;: 1, &apos;2&apos;: 2, &apos;3&apos;: 3, &apos;4&apos;: 4, &apos;5&apos;: 5, &apos;6&apos;: 6, &apos;7&apos;: 7, &apos;8&apos;: 8, &apos;9&apos;: 9}

def str2int(s):
    def fn(x, y):
        return x * 10 + y
    def char2num(s):
        return DIGITS[s]
    return reduce(fn, map(char2num, s))</code></pre><p>还可以用lambda函数进一步简化成：</p>
<pre><code>from functools import reduce

DIGITS = {&apos;0&apos;: 0, &apos;1&apos;: 1, &apos;2&apos;: 2, &apos;3&apos;: 3, &apos;4&apos;: 4, &apos;5&apos;: 5, &apos;6&apos;: 6, &apos;7&apos;: 7, &apos;8&apos;: 8, &apos;9&apos;: 9}

def char2num(s):
    return DIGITS[s]

def str2int(s):
    return reduce(lambda x, y: x * 10 + y, map(char2num, s))</code></pre><p>利用map()函数，把用户输入的不规范的英文名字，变为首字母大写，其他小写的规范名字。输入：[‘adam’, ‘LISA’, ‘barT’]，输出：[‘Adam’, ‘Lisa’, ‘Bart’]：</p>
<pre><code>def normal(name):
    return name.capitalize()

l=list(map(normal,[&apos;adam&apos;, &apos;LISA&apos;, &apos;barT&apos;]))
print(l)
[&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;]</code></pre><p>Python提供的sum()函数可以接受一个list并求和，请编写一个prod()函数，可以接受一个list并利用reduce()求积：</p>
<pre><code>from functools import reduce
def ji(x,y):
    return x*y

print(reduce(ji,[3,5,7,9]))</code></pre><p>利用map和reduce编写一个str2float函数，把字符串’123.456’转换成浮点数123.456：</p>
<pre><code>def str2float(s):
    DIGITS = {&apos;0&apos;: 0, &apos;1&apos;: 1, &apos;2&apos;: 2, &apos;3&apos;: 3, &apos;4&apos;: 4, &apos;5&apos;: 5, &apos;6&apos;: 6, &apos;7&apos;: 7, &apos;8&apos;: 8, &apos;9&apos;: 9}
    # 将传入的字符串转为list
    _list = list(s)
    # 求原list长度
    len_of_list = len(_list)
    # 求.的位置
    position_of_point = _list.index(&apos;.&apos;)
    # 删除.
    _list.pop(position_of_point)
    # 将str list转为int list
    _2_int_list = map(lambda x: DIGITS[x], _list)
    # 用reduce 将_2_int_list组合为int
    num = reduce(lambda x, y: 10 * x + y, _2_int_list)
    # 在适当位置添加小数点
    return num / 10**(len_of_list - (position_of_point + 1))
print(str2float(&apos;123.4242&apos;))</code></pre><p>##filter</p>
<p>Python内建的filter()函数用于过滤序列。</p>
<p>filter()也接收一个函数和一个序列。和map()不同的是，filter()把传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素。</p>
<p>例如，在一个list中，删掉偶数，只保留奇数，可以这么写：</p>
<pre><code>def is_odd(n):
    return n % 2 == 1

list(filter(is_odd, [1, 2, 4, 5, 6, 9, 10, 15]))
# 结果: [1, 5, 9, 15]</code></pre><p>把一个序列中的空字符串删掉，可以这么写：</p>
<pre><code>def not_empty(s):
    return s and s.strip()

list(filter(not_empty, [&apos;A&apos;, &apos;&apos;, &apos;B&apos;, None, &apos;C&apos;, &apos;  &apos;]))
# 结果: [&apos;A&apos;, &apos;B&apos;, &apos;C&apos;]</code></pre><p>用filter求素数<br>计算素数的一个方法是埃氏筛法，它的算法理解起来非常简单：</p>
<p>首先，列出从2开始的所有自然数，构造一个序列：</p>
<p>2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, …</p>
<p>取序列的第一个数2，它一定是素数，然后用2把序列的2的倍数筛掉：</p>
<p>3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, …</p>
<p>取新序列的第一个数3，它一定是素数，然后用3把序列的3的倍数筛掉：</p>
<p>5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, …</p>
<p>取新序列的第一个数5，然后用5把序列的5的倍数筛掉：</p>
<p>7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, …</p>
<p>不断筛下去，就可以得到所有的素数。</p>
<p>用Python来实现这个算法，可以先构造一个从3开始的奇数序列</p>
<pre><code>def _odd_iter():
    n = 1
    while True:
        n = n + 2
        yield n</code></pre><p>筛选函数：</p>
<pre><code>def _not_divisible(n):
    return lambda x: x % n &gt; 0</code></pre><p>最后，定义一个生成器，不断返回下一个素数：</p>
<pre><code>def primes():
    yield 2
    it = _odd_iter() # 初始序列
    while True:
        n = next(it) # 返回序列的第一个数
        yield n
        it = filter(_not_divisible(n), it) # 构造新序列</code></pre><p>这个生成器先返回第一个素数2，然后，利用filter()不断产生筛选后的新的序列。</p>
<p>由于primes()也是一个无限序列，所以调用时需要设置一个退出循环的条件：</p>
<pre><code># 打印1000以内的素数:
for n in primes():
    if n &lt; 1000:
        print(n)
    else:
        break</code></pre><p>##sorted 排序</p>
<p>排序也是在程序中经常用到的算法。无论使用冒泡排序还是快速排序，排序的核心是比较两个元素的大小。如果是数字，我们可以直接比较，但如果是字符串或者两个dict呢？直接比较数学上的大小是没有意义的，因此，比较的过程必须通过函数抽象出来。</p>
<p>Python内置的sorted()函数就可以对list进行排序</p>
<pre><code>&gt;&gt;&gt; sorted([36, 5, -12, 9, -21])
[-21, -12, 5, 9, 36]</code></pre><p>此外，sorted()函数也是一个高阶函数，它还可以接收一个key函数来实现自定义的排序，例如按绝对值大小排序：</p>
<pre><code>&gt;&gt;&gt; sorted([36, 5, -12, 9, -21], key=abs)
[5, 9, -12, -21, 36]</code></pre><p>我们再看一个字符串排序的例子，默认情况下，对字符串排序，是按照ASCII的大小比较的，由于’Z’ &lt; ‘a’，结果，大写字母Z会排在小写字母a的前面。现在，我们提出排序应该忽略大小写，按照字母序排序。要实现这个算法，不必对现有代码大加改动，只要我们能用一个key函数把字符串映射为忽略大小写排序即可。忽略大小写来比较两个字符串，实际上就是先把字符串都变成大写（或者都变成小写），再比较</p>
<pre><code>sorted([&apos;bob&apos;, &apos;about&apos;, &apos;Zoo&apos;, &apos;Credit&apos;], key=str.lower)
[&apos;about&apos;, &apos;bob&apos;, &apos;Credit&apos;, &apos;Zoo&apos;]</code></pre><p>要进行反向排序，不必改动key函数，可以传入第三个参数reverse=True：</p>
<pre><code>&gt;&gt;&gt; sorted([&apos;bob&apos;, &apos;about&apos;, &apos;Zoo&apos;, &apos;Credit&apos;], key=str.lower, reverse=True)
[&apos;Zoo&apos;, &apos;Credit&apos;, &apos;bob&apos;, &apos;about&apos;]




L = [(&apos;Bob&apos;, 75), (&apos;Adam&apos;, 92), (&apos;Bart&apos;, 66), (&apos;Lisa&apos;, 88)]
def by_name(t):
    return t[0]
def by_score(t):
    return t[1]
print(sorted(L,key=by_name))
print(sorted(L,key=by_score,reverse=True))</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/01/06/python%E5%AD%A6%E4%B9%A0/hello-world/" data-id="ck522ulm10000w4vjged5dobh" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-python学习/高级特性" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/05/python%E5%AD%A6%E4%B9%A0/%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7/" class="article-date">
  <time datetime="2020-01-05T05:17:01.586Z" itemprop="datePublished">2020-01-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/05/python%E5%AD%A6%E4%B9%A0/%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7/">高级特性</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="切片"><a href="#切片" class="headerlink" title="切片"></a>切片</h2><h3 id="begin"><a href="#begin" class="headerlink" title="begin"></a>begin</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">L = [&apos;Michael&apos;, &apos;Sarah&apos;, &apos;Tracy&apos;, &apos;Bob&apos;, &apos;Jack&apos;]</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017269965565856" target="_blank" rel="noopener">原文</a></p>
<h3 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[L[0], L[1], L[2]]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&apos;Michael&apos;, &apos;Sarah&apos;, &apos;Tracy&apos;]</span><br></pre></td></tr></table></figure>


<h3 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h3><pre><code>r = []
n = 3 
for i in range(n):
    r.append(L[i])
r

[&apos;Michael&apos;, &apos;Sarah&apos;, &apos;Tracy&apos;]</code></pre><h3 id="方法三"><a href="#方法三" class="headerlink" title="方法三"></a>方法三</h3><p>L[0:3]表示，从索引0开始取，直到索引3为止，但不包括索引3。即索引0，1，2，正好是3个元素。</p>
<p>如果第一个索引是0，还可以省略：</p>
<pre><code>&gt;&gt;&gt; L[0:3]
[&apos;Michael&apos;, &apos;Sarah&apos;, &apos;Tracy&apos;]
&gt;&gt;&gt; L[1:3]
[&apos;Sarah&apos;, &apos;Tracy&apos;]</code></pre><p>类似的，既然Python支持L[-1]取倒数第一个元素，那么它同样支持倒数切片，试试：</p>
<pre><code>&gt;&gt;&gt; L[-2:]
[&apos;Bob&apos;, &apos;Jack&apos;]
&gt;&gt;&gt; L[-2:-1]
[&apos;Bob&apos;]</code></pre><p>新建一个1到100的列表：</p>
<pre><code>&gt;&gt;&gt; L = list(range(100))
&gt;&gt;&gt; L
[0, 1, 2, 3, ..., 99]</code></pre><p>所有数，每5个取一个：</p>
<pre><code>&gt;&gt;&gt; L[::5]
[0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95]</code></pre><p>利用切片去除首尾的空格： </p>
<pre><code>def(s)
    while s[:1]==&apos; &apos;:
        s= s[1:]
    while s[-1:]==&apos; &apos;:
        s= s[:-1]

    return s</code></pre><p>##迭代</p>
<p>###方法一<br>对列表进行迭代：</p>
<pre><code>for (i=0; i&lt;list.length; i++) {
    n = list[i];
}</code></pre><p>对字典进行迭代：</p>
<pre><code>&gt;&gt;&gt; d = {&apos;a&apos;: 1, &apos;b&apos;: 2, &apos;c&apos;: 3}
&gt;&gt;&gt; for key in d:
...     print(key)
...
a
c
b</code></pre><p>判断是否可以迭代：</p>
<pre><code>&gt;&gt;&gt; from collections import Iterable
&gt;&gt;&gt; isinstance(&apos;abc&apos;, Iterable) # str是否可迭代
True
&gt;&gt;&gt; isinstance([1,2,3], Iterable) # list是否可迭代
True
&gt;&gt;&gt; isinstance(123, Iterable) # 整数是否可迭代
False    </code></pre><p>Python内置的<strong>enumerate</strong>函数可以把一个list变成索引-元素对，这样就可以在for循环中同时迭代索引和元素本身</p>
<pre><code>&gt;&gt;&gt; for i, value in enumerate([&apos;A&apos;, &apos;B&apos;, &apos;C&apos;]):
...     print(i, value)
...
0 A
1 B

&gt;&gt;&gt; for x, y in [(1, 1), (2, 4), (3, 9)]:
...     print(x, y)
...
1 1
2 4
3 9</code></pre><p>找出最大值和最小值：</p>
<pre><code>def findMinAndMax(L):

    if L==[]:

    print((None,None))

    else:

    min=L[0]

    max=L[0]

    for i,value in enumerate(L):

        if value&lt;min:

        min=L[i]

        if value&gt;max:

        max=L[i]

    return (min,max)</code></pre><p>##列表生成</p>
<p>###方法</p>
<p>生成有序一元组：</p>
<pre><code>&gt;&gt;&gt; list(range(1, 11))
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

&gt;&gt;&gt; L = []
&gt;&gt;&gt; for x in range(1, 11):
...    L.append(x * x)
...
&gt;&gt;&gt; L
[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]</code></pre><p>在for循环中加入函数：</p>
<pre><code>&gt;&gt;&gt; [x * x for x in range(1, 11)]
[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]</code></pre><p>在for循环中加入条件：</p>
<pre><code>&gt;&gt;&gt; [x * x for x in range(1, 11) if x % 2 == 0]
[4, 16, 36, 64, 100]</code></pre><p>还可以使用两层循环，可以生成全排列：</p>
<pre><code>&gt;&gt;&gt; [m + n for m in &apos;ABC&apos; for n in &apos;XYZ&apos;]
[&apos;AX&apos;, &apos;AY&apos;, &apos;AZ&apos;, &apos;BX&apos;, &apos;BY&apos;, &apos;BZ&apos;, &apos;CX&apos;, &apos;CY&apos;, &apos;CZ&apos;]</code></pre><p>列出当前目录下的所有文件和目录名，可以通过一行代码实现</p>
<pre><code>&gt;&gt;&gt; import os # 导入os模块，模块的概念后面讲到
&gt;&gt;&gt; [d for d in os.listdir(&apos;.&apos;)] # os.listdir可以列出文件和目录</code></pre><p>for循环其实可以同时使用两个甚至多个变量，比如dict的items()可以同时迭代key和value：</p>
<pre><code>&gt;&gt;&gt; d = {&apos;x&apos;: &apos;A&apos;, &apos;y&apos;: &apos;B&apos;, &apos;z&apos;: &apos;C&apos; }
&gt;&gt;&gt; for k, v in d.items():
...     print(k, &apos;=&apos;, v)
...
y = B
x = A
z = C</code></pre><p>因此，列表生成式也可以使用两个变量来生成list：</p>
<pre><code>&gt;&gt;&gt; d = {&apos;x&apos;: &apos;A&apos;, &apos;y&apos;: &apos;B&apos;, &apos;z&apos;: &apos;C&apos; }
&gt;&gt;&gt; [k + &apos;=&apos; + v for k, v in d.items()]
[&apos;y=B&apos;, &apos;x=A&apos;, &apos;z=C&apos;]

&gt;&gt;&gt; L = [&apos;Hello&apos;, &apos;World&apos;, &apos;IBM&apos;, &apos;Apple&apos;]
&gt;&gt;&gt; [s.lower() for s in L]
[&apos;hello&apos;, &apos;world&apos;, &apos;ibm&apos;, &apos;apple&apos;]</code></pre><p>清楚L1中的非字符，并且将字符串转化为小写。</p>
<pre><code>L2 = [s.lower() for s in L1 if isinstance(s, str)]    </code></pre><p>##生成器</p>
<p>如果列表元素可以按照某种算法推算出来，那我们是否可以在循环的过程中不断推算出后续的元素呢？这样就不必创建完整的list，从而节省大量的空间。在Python中，这种一边循环一边计算的机制，称为生成器：generator</p>
<p>要创建一个generator，有很多种方法。第一种方法很简单，只要把一个列表生成式的[]改成()，就创建了一个generator：</p>
<pre><code>&gt;&gt;&gt; L = [x * x for x in range(10)]
&gt;&gt;&gt; L
[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
&gt;&gt;&gt; g = (x * x for x in range(10))
&gt;&gt;&gt; g
&lt;generator object &lt;genexpr&gt; at 0x1022ef630&gt;</code></pre><p>如果要一个一个打印出来，可以通过next()函数获得generator的下一个返回值：</p>
<pre><code>&gt;&gt;&gt; next(g)
0
&gt;&gt;&gt; next(g)
1
&gt;&gt;&gt; next(g)
4
&gt;&gt;&gt; next(g)
9
&gt;&gt;&gt; next(g)
16
&gt;&gt;&gt; next(g)
25
&gt;&gt;&gt; next(g)
36
&gt;&gt;&gt; next(g)
49
&gt;&gt;&gt; next(g)
64
&gt;&gt;&gt; next(g)
81
&gt;&gt;&gt; next(g)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
StopIteration</code></pre><p>当然，上面这种不断调用next(g)实在是太变态了，正确的方法是使用for循环，因为generator也是可迭代对象</p>
<pre><code>&gt;&gt;&gt; g = (x * x for x in range(10))
&gt;&gt;&gt; for n in g:
...     print(n)
... 
0
1
4
9
16
25
36
49
64
81</code></pre><p>斐波拉契数列用列表生成式写不出来，但是，用函数把它打印出来却很容易：</p>
<pre><code>def fib(max):
    n, a, b = 0, 0, 1
    while n &lt; max:
        print(b)
        a, b = b, a + b
        n = n + 1
    return &apos;done&apos;</code></pre><p>也就是说，上面的函数和generator仅一步之遥。要把fib函数变成generator，只需要把print(b)改为yield b就可以了：</p>
<pre><code>def fib(max):
    n, a, b = 0, 0, 1
    while n &lt; max:
        yield b
        a, b = b, a + b
        n = n + 1
    return &apos;done&apos;</code></pre><p>这就是定义generator的另一种方法。如果一个函数定义中包含yield关键字，那么这个函数就不再是一个普通函数，而是一个generator：</p>
<pre><code>&gt;&gt;&gt; f = fib(6)
&gt;&gt;&gt; f
&lt;generator object fib at 0x104feaaa0&gt;</code></pre><p>这里，最难理解的就是generator和函数的执行流程不一样。函数是顺序执行，遇到return语句或者最后一行函数语句就返回。而变成generator的函数，在每次调用next()的时候执行，遇到yield语句返回，再次执行时从上次返回的yield语句处继续执行。</p>
<pre><code>def odd():
    print(&apos;step 1&apos;)
    yield 1
    print(&apos;step 2&apos;)
    yield(3)
    print(&apos;step 3&apos;)
    yield(5)</code></pre><p>调用该generator时，首先要生成一个generator对象，然后用next()函数不断获得下一个返回值：</p>
<pre><code>&gt;&gt;&gt; o = odd()
&gt;&gt;&gt; next(o)
step 1
1
&gt;&gt;&gt; next(o)
step 2
3
&gt;&gt;&gt; next(o)
step 3
5
&gt;&gt;&gt; next(o)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
StopIteration</code></pre><p>但是用for循环调用generator时，发现拿不到generator的return语句的返回值。如果想要拿到返回值，必须捕获StopIteration错误，返回值包含在StopIteration的value中：</p>
<pre><code>&gt;&gt;&gt; g = fib(6)
&gt;&gt;&gt; while True:
...     try:
...         x = next(g)
...         print(&apos;g:&apos;, x)
...     except StopIteration as e:
...         print(&apos;Generator return value:&apos;, e.value)
...         break
...
g: 1
g: 1
g: 2
g: 3
g: 5
g: 8
Generator return value: done</code></pre><p>生成杨辉三角：</p>
<pre><code>def triangles():
    L = [1]
    while True:
        yield L
        L = [L[i] + L[i + 1] for i in range(len(L) - 1)]
        L.append(1)
        L.insert(0, 1)</code></pre><p>##迭代器：</p>
<p>可以使用isinstance()判断一个对象是否是Iterable对象：</p>
<pre><code>&gt;&gt;&gt; from collections import Iterable
&gt;&gt;&gt; isinstance([], Iterable)
True
&gt;&gt;&gt; isinstance({}, Iterable)
True
&gt;&gt;&gt; isinstance(&apos;abc&apos;, Iterable)
True
&gt;&gt;&gt; isinstance((x for x in range(10)), Iterable)
True
&gt;&gt;&gt; isinstance(100, Iterable)
False</code></pre><p>可以被next()函数调用并不断返回下一个值的对象称为迭代器：Iterator。</p>
<p>可以使用isinstance()判断一个对象是否是Iterator对象：</p>
<pre><code>&gt;&gt;&gt; from collections import Iterator
&gt;&gt;&gt; isinstance((x for x in range(10)), Iterator)
True
&gt;&gt;&gt; isinstance([], Iterator)
False
&gt;&gt;&gt; isinstance({}, Iterator)
False
&gt;&gt;&gt; isinstance(&apos;abc&apos;, Iterator)
False</code></pre><p>生成器都是Iterator对象，但list、dict、str虽然是Iterable，却不是Iterator。</p>
<p>把list、dict、str等Iterable变成Iterator可以使用iter()函数：</p>
<pre><code>&gt;&gt;&gt; isinstance(iter([]), Iterator)
True
&gt;&gt;&gt; isinstance(iter(&apos;abc&apos;), Iterator)
True</code></pre><p>这是因为Python的Iterator对象表示的是一个数据流，Iterator对象可以被next()函数调用并不断返回下一个数据，直到没有数据时抛出StopIteration错误。可以把这个数据流看做是一个有序序列，但我们却不能提前知道序列的长度，只能不断通过next()函数实现按需计算下一个数据，所以Iterator的计算是惰性的，只有在需要返回下一个数据时它才会计算。</p>
<p>Iterator甚至可以表示一个无限大的数据流，例如全体自然数。而使用list是永远不可能存储全体自然数的</p>
<pre><code># 首先获得Iterator对象:
it = iter([1, 2, 3, 4, 5])
# 循环:
while True:
    try:
        # 获得下一个值:
        x = next(it)
    except StopIteration:
        # 遇到StopIteration就退出循环
    break</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/01/05/python%E5%AD%A6%E4%B9%A0/%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7/" data-id="ck50naksc000bc8vjfzqd3on6" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-MDS降维" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/12/03/MDS%E9%99%8D%E7%BB%B4/" class="article-date">
  <time datetime="2019-12-03T12:04:53.837Z" itemprop="datePublished">2019-12-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/03/MDS%E9%99%8D%E7%BB%B4/">MDS</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a><strong>算法介绍</strong></h3><p>非常传统的降维的方法，以距离为标准，将高维坐标中的点投影到低维坐标中，保持彼此之间的相对距离变化最小，更新的方法是T-SNE，基于分布概率变化最小进行投影。</p>
<p>假定原始高维数据样本的距离矩阵为D，则在低维下的距离矩阵为Z，我们可以用优化算法选取初始点，用梯度下降法求最佳逼近，使得||D-Z||最小，同时，也可以利用內积来求的低维映射。前者在样本较多时容易陷入局部最优，后者较稳定，但在样本不多时，效果比前者要差。</p>
<p>代码如下：这里用了两种数据集：球形数据集合Iris数据集，算法分别用了基于上述內积求优的算法和sklearn中基于梯度下降求最优的算法，并进行了比较，总体来说，后者稳定，前者更快速。</p>
<pre><code># coding:utf-8
import numpy as np
from sklearn.datasets import load_iris
from sklearn.manifold import MDS
import matplotlib.pyplot as plt

&apos;&apos;&apos;
author: heucoder
email: 812860165@qq.com
date: 2019.6.13
&apos;&apos;&apos;

def cal_pairwise_dist(x):
    &apos;&apos;&apos;计算pairwise 距离, x是matrix
    (a-b)^2 = a^2 + b^2 - 2*a*b
    &apos;&apos;&apos;
    sum_x = np.sum(np.square(x), 1)
    dist = np.add(np.add(-2 * np.dot(x, x.T), sum_x).T, sum_x)
    #返回任意两个点之间距离的平方
    return dist


def my_mds(data, n_dims):
    &apos;&apos;&apos;
    :param data: (n_samples, n_features)
    :param n_dims: target n_dims
    :return: (n_samples, n_dims)
    &apos;&apos;&apos;

    n, d = data.shape
    dist = cal_pairwise_dist(data)
    T1 = np.ones((n,n))*np.sum(dist)/n**2
    T2 = np.sum(dist, axis = 1, keepdims=True)/n
    T3 = np.sum(dist, axis = 0, keepdims=True)/n

    B = -(T1 - T2 - T3 + dist)/2

    eig_val, eig_vector = np.linalg.eig(B)
    index_ = np.argsort(-eig_val)[:n_dims]
    picked_eig_val = eig_val[index_].real
    picked_eig_vector = eig_vector[:, index_]
    # print(picked_eig_vector.shape, picked_eig_val.shape)
    return picked_eig_vector*picked_eig_val**(0.5)

if __name__ == &apos;__main__&apos;:
    iris = load_iris()
    data = iris.data
    Y = iris.target
    data_1 = my_mds(data, 2)

    data_2 = MDS(n_components=2).fit_transform(data)

    plt.figure(figsize=(8, 4))
    plt.subplot(121)
    plt.title(&quot;my_MDS&quot;)
    plt.scatter(data_1[:, 0], data_1[:, 1], c=Y)

    plt.subplot(122)
    plt.title(&quot;sklearn_MDS&quot;)
    plt.scatter(data_2[:, 0], data_2[:, 1], c=Y)
    plt.savefig(&quot;MDS_1.png&quot;)
    plt.show()</code></pre><p><img src="https://i.imgur.com/GrDFoIi.png" alt=""></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/12/03/MDS%E9%99%8D%E7%BB%B4/" data-id="ck50nakqw0001c8vjfh4r8gjo" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-关键词提取算法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/12/01/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96%E7%AE%97%E6%B3%95/" class="article-date">
  <time datetime="2019-12-01T07:20:34.587Z" itemprop="datePublished">2019-12-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/01/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96%E7%AE%97%E6%B3%95/">关键词提取</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="技术概要"><a href="#技术概要" class="headerlink" title="技术概要"></a>技术概要</h2><p>描述：如果我们可以准确地将所有的文档都用几个简单的关键词描述出来，我们单单看这几个关键词就可以判断这篇文章是不是我们所需要的。这样就可以提高我们信息获取的效率。</p>
<p>###有监督和无监督<br><strong>有监督</strong>：通过分类的方式进行的，首先，通过构建一个较为丰富和完善的词表，然后通过判断每个文档与词表中的每个词的匹配程度，以这种类似打标签的方法，达到关键词提取的效果。</p>
<p>优点：有较高的精度<br>缺点：需要大批量的标注的数据，人工成不过高。</p>
<p><strong>无监督</strong>：TF-IDF,TextRank算法，主题模型算法（LSA, LSI,LDA)</p>
<h2 id="TF-IDF算法"><a href="#TF-IDF算法" class="headerlink" title="TF/IDF算法"></a>TF/IDF算法</h2><p><strong>TF/IDF算法</strong>（Term Frequency-Inverse Document Frequency),词频-逆文档频次算法。）是一种基于统计的及算法方法。用于评估在一个文档中一个词对某个文档的重要程度。</p>
<p><strong>TF算法：</strong></p>
<p>TF算法是统计一个词在一篇文档中出现的频次，其思想可以这样描述：一个词在文档中出现的次数越多，则对文档的表达能力就越强。<br><img src="https://cdn.mathpix.com/snip/images/eCvWPEPHVDuGuoly8pfa5UysNHY1NvjKl7eDw4gTJgI.original.fullsize.png" alt=""></p>
<p>tf（word）=（word在文档中出现的次数）/（文档总词数）</p>
<p><strong>例子：</strong></p>
<blockquote>
<pre><code>世界献血日，学校团体、献血服务自愿者等可到血液中心参观检验加工过程，我们会对检验结果进行公示，同时血液的价格也会进行公示。</code></pre><p>上文中“献血”，“血液”，“进行”，“公示”等词出现的频次均为2.</p>
</blockquote>
<p><strong>IDF</strong>：<br>统计一个词在文档集中的多少个文档中出现，描述：如果一个词在越少的文档中出现，则其对文档的区分能力就越强<br><img src="https://cdn.mathpix.com/snip/images/xVxByO7dPrxtjRa0HDgYjjUY6g5pcmXsh00JnNvIQLM.original.fullsize.png" alt=""></p>
<p>|D|是文档集中的总文档数 ，|Di|是文档集中出现词i的文档数量。分母加一是采用拉普拉斯平滑，避免有部分新的词没有在语料库中出现过而导致分母为零，增强算法的健壮性。</p>
<p><strong>tfxIdf(i,j)</strong></p>
<p><img src="https://cdn.mathpix.com/snip/images/tyaMpPhnWCSpNJegpmGpjHbsA5-PrNrrKOMtPKAwFEg.original.fullsize.png" alt=""></p>
<p>如上述文档中tf=2/30=0.067。我们假设文档集有1000.其中出现“血液”，“献血”，为15，10.</p>
<p>所以idf（献血）=10  idf（血液）=4.2</p>
<p>##TextRank算法</p>
<p><strong>描述：</strong>TextRank算法则是可以脱离语料库的背景，仅对单个文档进行分析来提取关键词。基于句子的维度分析，给每个句子进行打分，挑出分数最高的n个句子作为文档的关键句。</p>
<p>###PageRank</p>
<p>PageRank算法是Google创始人拉里.佩奇和谢尔盖.布林于1997年构建早期的搜索系统原型是提出的链接分析算法，该算法是他们用来评价搜索系统网页重要性的一种算法。</p>
<p>基本思想：</p>
<p>1）链接数量。一个网页被越多的其他网页链接，说明这个网页越重要。</p>
<p>2）链接质量。一个网页被一个越高权值的网页链接，也能表明这个网页越重要。</p>
<p><img src="https://cdn.mathpix.com/snip/images/TOdNlF5s-sh_JGMr3sTRwPNDp91eu7O4GfJQbdBIlD4.original.fullsize.png" alt=""></p>
<p>算法开始时会将所有网页的得分初始化为1，来对每个网页的分数进行收敛。</p>
<p>上式中，会导致一些孤立网页（也就是没有出入链的网页）可能得分为零。因此加入一个阻尼系数d。</p>
<p><img src="https://cdn.mathpix.com/snip/images/6XV1HV1WBn7X_oBlvwKGzFn_HzHhE4HX_Pt8Z4mwUBU.original.fullsize.png" alt=""></p>
<p>###TextRank<br>TextRank 是在PageRank的基础上修改的。</p>
<p>TextRank进行自动摘要是有权图。因为在计分时除了考虑链接句的重要性外，还要考虑两个句子间的相似性。<br>计算每个句子给它链接句的贡献时，不是通过平均分配的方式，而是通过计算权重占总权重的比例分配的。在这里权重就是两个句子的相似度。</p>
<p>相似度计算可以采用编辑距离，余弦相似度等。</p>
<p><img src="https://cdn.mathpix.com/snip/images/GAbrYgPP7ctBZhLxEkHfzgDsfuiBNGIsgxSgKUCRJdo.original.fullsize.png" alt=""></p>
<p>##主题模型算法</p>
<p>若有一篇文章中介绍了狮子，老虎，狗，猫等动物，但是文章中没有动物两个字，so，前面两种算法就不能提取出动物这个隐含的主题信息。</p>
<p>###LSA（LSI）<br>LSA(Latent Semantic Analysis,潜在语义分析）LSI（Latent Semantic index,潜在语义索引）</p>
<p>LSA算法步骤：</p>
<p>1）使用BOW模型将每个文档表示为向量</p>
<p>2）将文档词向量拼接起来构成词-文档矩阵（mxn）；</p>
<p>3） 使用奇异值分解（SVD） 操作（[mxr].[rxr].[r*n])</p>
<p>4) 将文档映射到更低维度k（[mxk].[kxk] .[k*n])</p>
<p>每个词和文档都可以表示为k个主题构成的空间中的一个点，通过计算每个词和文档的相似度（余弦相似度orKl相似度），得到每个文档中的每个词的相似度结果。取相似度最高的词即为关键词。</p>
<p>SVD的计算复杂度非常高，特征空间维度较大，计算效率十分低。</p>
<p>###LDA算法</p>
<p>1 对d维数据进行标准化处理（d为特征数量）</p>
<p>2 对于每一类别，计算d维的均值向量</p>
<p>3 构造类间的散布矩阵 SBSB 以及 类内散布矩阵 SWSW</p>
<p>4 计算矩阵 S−1WSBSW−1SB 的特征值以及对应的特征向量</p>
<p>5 选取前k个特征值所对应的特征向量，构造一个 d∗kd∗k 维的转换矩阵 WW,其中特征向量以列的形式排列</p>
<p>6 使用转换矩阵 WW 将样本映射到新的特征子空间上</p>
<p><strong>代码：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">author: heucoder</span></span><br><span class="line"><span class="string">email: 812860165@qq.com</span></span><br><span class="line"><span class="string">date: 2019.6.13</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lda</span><span class="params">(data, target, n_dim)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    :param data: (n_samples, n_features)</span></span><br><span class="line"><span class="string">    :param target: data class</span></span><br><span class="line"><span class="string">    :param n_dim: target dimension</span></span><br><span class="line"><span class="string">    :return: (n_samples, n_dims)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    clusters = np.unique(target)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> n_dim &gt; len(clusters) - <span class="number">1</span>:</span><br><span class="line">        print(<span class="string">"K is too much"</span>)</span><br><span class="line">        print(<span class="string">"please input again"</span>)</span><br><span class="line">        exit(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># within_class scatter matrix</span></span><br><span class="line">    Sw = np.zeros((data.shape[<span class="number">1</span>], data.shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> clusters:</span><br><span class="line">        datai = data[target == i]</span><br><span class="line">        datai = datai - datai.mean(<span class="number">0</span>)</span><br><span class="line">        Swi = np.mat(datai).T * np.mat(datai)</span><br><span class="line">        Sw += Swi</span><br><span class="line"></span><br><span class="line">    <span class="comment"># between_class scatter matrix</span></span><br><span class="line">    SB = np.zeros((data.shape[<span class="number">1</span>], data.shape[<span class="number">1</span>]))</span><br><span class="line">    u = data.mean(<span class="number">0</span>)  <span class="comment"># 所有样本的平均值</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> clusters:</span><br><span class="line">        Ni = data[target == i].shape[<span class="number">0</span>]</span><br><span class="line">        ui = data[target == i].mean(<span class="number">0</span>)  <span class="comment"># 某个类别的平均值</span></span><br><span class="line">        SBi = Ni * np.mat(ui - u).T * np.mat(ui - u)</span><br><span class="line">        SB += SBi</span><br><span class="line">    S = np.linalg.inv(Sw) * SB</span><br><span class="line">    eigVals, eigVects = np.linalg.eig(S)  <span class="comment"># 求特征值，特征向量</span></span><br><span class="line">    eigValInd = np.argsort(eigVals)</span><br><span class="line">    eigValInd = eigValInd[:(-n_dim - <span class="number">1</span>):<span class="number">-1</span>]</span><br><span class="line">    w = eigVects[:, eigValInd]</span><br><span class="line">    data_ndim = np.dot(data, w)</span><br><span class="line">    print(data_ndim)</span><br><span class="line">    <span class="keyword">return</span> data_ndim</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    iris = load_iris()</span><br><span class="line">    X = iris.data</span><br><span class="line">    Y = iris.target</span><br><span class="line">    data_1 = lda(X, Y, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    data_2 = LinearDiscriminantAnalysis(n_components=<span class="number">2</span>).fit_transform(X, Y)</span><br><span class="line"></span><br><span class="line">    plt.figure(figsize=(<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line">    plt.subplot(<span class="number">121</span>)</span><br><span class="line">    plt.title(<span class="string">"my_LDA"</span>)</span><br><span class="line">    plt.scatter(data_1[:, <span class="number">0</span>], data_1[:, <span class="number">1</span>], c=Y)</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">122</span>)</span><br><span class="line">    plt.title(<span class="string">"sklearn_LDA"</span>)</span><br><span class="line">    plt.scatter(data_2[:, <span class="number">0</span>], data_2[:, <span class="number">1</span>], c=Y)</span><br><span class="line">    plt.savefig(<span class="string">"LDA.png"</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/12/01/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96%E7%AE%97%E6%B3%95/" data-id="ck50nakrj0007c8vjevbq8o8x" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-KPCA线性降维算法 " class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/12/01/KPCA%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%20/" class="article-date">
  <time datetime="2019-12-01T05:41:39.418Z" itemprop="datePublished">2019-12-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/01/KPCA%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%20/">KPCA</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="特征抽取"><a href="#特征抽取" class="headerlink" title="特征抽取"></a>特征抽取</h2><p>可以将原始数据集变换到一个维度更低的新的特征子空间，在尽可能多地保持相关信息的情况下，对数据进行压缩</p>
<p>##线性可分 VS 非线性可分</p>
<p><img src="https://github.com/rasbt/python-machine-learning-book/raw/master/code/ch05/images/05_11.png" alt="fsaf"></p>
<p>可以通过kPCA将非线性数据映射到高维空间，在高维空间下使用标准PCA将其映射到另一个低维空间</p>
<p><img src="https://github.com/KARL13YAN/my-blogs/raw/master/pythonML/kPCA.png" alt=""></p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>定义非线性映射函数，该函数可以对原始特征进行非线性组合，以将原始的d维数据集映射到更高维的k维特征空间。</p>
<p>##使用Python实现kPCA<br>    from scipy.spatial.distance import pdist, squareform<br>    from scipy import exp<br>    from numpy.linalg import eigh<br>    import numpy as np</p>
<pre><code>def rbf_kernel_pca(X, gamma, n_components):
    &quot;&quot;&quot;
    RBF kernel PCA implementation.

    Parameters
    ------------
    X: {NumPy ndarray}, shape = [n_samples, n_features]

    gamma: float
      Tuning parameter of the RBF kernel

    n_components: int
      Number of principal components to return

    Returns
    ------------
     X_pc: {NumPy ndarray}, shape = [n_samples, k_features]
       Projected dataset   

    &quot;&quot;&quot;
    # Calculate pairwise squared Euclidean distances
    # in the MxN dimensional dataset.
    sq_dists = pdist(X, &apos;sqeuclidean&apos;)

    # Convert pairwise distances into a square matrix.
    mat_sq_dists = squareform(sq_dists)

    # Compute the symmetric kernel matrix.
    K = exp(-gamma * mat_sq_dists)

    # Center the kernel matrix.
    N = K.shape[0]
    one_n = np.ones((N, N)) / N
    K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)

    # Obtaining eigenpairs from the centered kernel matrix
    # numpy.linalg.eigh returns them in sorted order
    eigvals, eigvecs = eigh(K)

    # Collect the top k eigenvectors (projected samples)
    X_pc = np.column_stack((eigvecs[:, -i]
                            for i in range(1, n_components + 1)))

    return X_pc</code></pre><h2 id="创建如下数据"><a href="#创建如下数据" class="headerlink" title="创建如下数据"></a>创建如下数据</h2><pre><code>import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
X, y = make_moons(n_samples=100, random_state=123)
plt.scatter(X[y == 0, 0], X[y == 0, 1], color=&apos;red&apos;, marker=&apos;^&apos;, alpha=0.5)
plt.scatter(X[y == 1, 0], X[y == 1, 1], color=&apos;blue&apos;, marker=&apos;o&apos;, alpha=0.5)

plt.tight_layout()
plt.show()</code></pre><h2 id="使用PCA算法"><a href="#使用PCA算法" class="headerlink" title="使用PCA算法"></a>使用PCA算法</h2><pre><code>from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

scikit_pca = PCA(n_components=2)
X_spca = scikit_pca.fit_transform(X)

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))

ax[0].scatter(X_spca[y == 0, 0], X_spca[y == 0, 1],
              color=&apos;red&apos;, marker=&apos;^&apos;, alpha=0.5)
ax[0].scatter(X_spca[y == 1, 0], X_spca[y == 1, 1],
              color=&apos;blue&apos;, marker=&apos;o&apos;, alpha=0.5)

ax[1].scatter(X_spca[y == 0, 0], np.zeros((50, 1)) + 0.02,
              color=&apos;red&apos;, marker=&apos;^&apos;, alpha=0.5)
ax[1].scatter(X_spca[y == 1, 0], np.zeros((50, 1)) - 0.02,
              color=&apos;blue&apos;, marker=&apos;o&apos;, alpha=0.5)

ax[0].set_xlabel(&apos;PC1&apos;)
ax[0].set_ylabel(&apos;PC2&apos;)
ax[1].set_ylim([-1, 1])
ax[1].set_yticks([])
ax[1].set_xlabel(&apos;PC1&apos;)

plt.tight_layout()
# plt.savefig(&apos;./figures/half_moon_2.png&apos;, dpi=300)
plt.show()</code></pre><h2 id="使用KPCA算法"><a href="#使用KPCA算法" class="headerlink" title="使用KPCA算法"></a>使用KPCA算法</h2><pre><code>from matplotlib.ticker import FormatStrFormatter

X_kpca = rbf_kernel_pca(X, gamma=15, n_components=2)

fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(7,3))
ax[0].scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], 
            color=&apos;red&apos;, marker=&apos;^&apos;, alpha=0.5)
ax[0].scatter(X_kpca[y==1, 0], X_kpca[y==1, 1],
            color=&apos;blue&apos;, marker=&apos;o&apos;, alpha=0.5)

ax[1].scatter(X_kpca[y==0, 0], np.zeros((50,1))+0.02, 
            color=&apos;red&apos;, marker=&apos;^&apos;, alpha=0.5)
ax[1].scatter(X_kpca[y==1, 0], np.zeros((50,1))-0.02,
            color=&apos;blue&apos;, marker=&apos;o&apos;, alpha=0.5)

ax[0].set_xlabel(&apos;PC1&apos;)
ax[0].set_ylabel(&apos;PC2&apos;)
ax[1].set_ylim([-1, 1])
ax[1].set_yticks([])
ax[1].set_xlabel(&apos;PC1&apos;)
ax[0].xaxis.set_major_formatter(FormatStrFormatter(&apos;%0.1f&apos;))
ax[1].xaxis.set_major_formatter(FormatStrFormatter(&apos;%0.1f&apos;))

plt.tight_layout()
# plt.savefig(&apos;./figures/half_moon_3.png&apos;, dpi=300)
plt.show()</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/12/01/KPCA%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%20/" data-id="ck50nakqo0000c8vj8qtvadge" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-PCA线性降维算法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/12/01/PCA%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95/" class="article-date">
  <time datetime="2019-12-01T04:51:06.192Z" itemprop="datePublished">2019-12-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/01/PCA%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95/">PCA</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="协方差Cov（x，y）"><a href="#协方差Cov（x，y）" class="headerlink" title="协方差Cov（x，y）"></a>协方差Cov（x，y）</h2><p>COV(X,Y)=E(XY)-E(X)E(Y)<br>COV(X,Y)=[D(X)+D(Y)-D(X+Y)]/2</p>
<h2 id="PCA概念"><a href="#PCA概念" class="headerlink" title="PCA概念"></a>PCA概念</h2><p>PCA 是一种基于从高维空间映射到低维空间的映射方法，也是最基础的无监督降维算法，其目标是向数据变化最大的方向投影，或者说向重构误差最小化的方向投影。它由 Karl Pearson 在 1901 年提出，属于线性降维方法。与 PCA 相关的原理通常被称为最大方差理论或最小误差理论。这两者目标一致，但过程侧重点则不同。</p>
<h2 id="最大方法降维原理"><a href="#最大方法降维原理" class="headerlink" title="最大方法降维原理"></a>最大方法降维原理</h2><p>将一组 N 维向量降为 K 维（K 大于 0，小于 N），其目标是选择 K 个单位正交基，各字段两两间 COV(X,Y) 为 0，而字段的方差则尽可能大。因此，最大方差即使得投影数据的方差被最大化，在这过程中，我们需要找到数据集 Xmxn 的最佳的投影空间 Wnxk、协方差矩阵等，其算法流程为<br>算法输入：数据集 Xmxn；</p>
<p>按列计算数据集 X 的均值 Xmean，然后令 Xnew=XXmean；</p>
<p>求解矩阵 Xnew 的协方差矩阵，并将其记为 Cov；</p>
<p>计算协方差矩阵 COv 的特征值和相应的特征向量；</p>
<p>将特征值按照从大到小的排序，选择其中最大的 k 个，然后将其对应的 k 个特征向量分别作为列向量组成特征向量矩阵 Wnxk；</p>
<p>计算 XnewW，即将数据集 Xnew 投影到选取的特征向量上，这样就得到了我们需要的已经降维的数据集 XnewW。</p>
<h2 id="PCA算法优缺点"><a href="#PCA算法优缺点" class="headerlink" title="PCA算法优缺点"></a>PCA算法优缺点</h2><p>优点：</p>
<p>它是无监督学习，完全无参数限制的。在PCA的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。</p>
<p>用PCA技术可以对数据进行降维，同时对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。</p>
<p>各主成分之间正交，可消除原始数据成分间的相互影响。</p>
<p>计算方法简单，易于在计算机上实现。</p>
<p>缺点：</p>
<p>如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。</p>
<p>贡献率小的主成分往往可能含有对样本差异的重要信息。</p>
<p>特征值矩阵的正交向量空间是否唯一有待讨论。</p>
<p>在非高斯分布的情况下，PCA方法得出的主元可能并不是最优的，此时在寻找主元时不能将方差作为衡量重要性的标准。</p>
<h2 id="python算法实现"><a href="#python算法实现" class="headerlink" title="python算法实现"></a>python算法实现</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.cm <span class="keyword">as</span> cmx</span><br><span class="line"><span class="keyword">import</span> matplotlib.colors <span class="keyword">as</span> colors</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shuffle_data</span><span class="params">(X, y, seed=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> seed:</span><br><span class="line">        np.random.seed(seed)</span><br><span class="line"></span><br><span class="line">    idx = np.arange(X.shape[<span class="number">0</span>])</span><br><span class="line">    np.random.shuffle(idx)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X[idx], y[idx]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 正规化数据集 X</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span><span class="params">(X, axis=<span class="number">-1</span>, p=<span class="number">2</span>)</span>:</span></span><br><span class="line">    lp_norm = np.atleast_1d(np.linalg.norm(X, p, axis))</span><br><span class="line">    lp_norm[lp_norm == <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> X / np.expand_dims(lp_norm, axis)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化数据集 X</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standardize</span><span class="params">(X)</span>:</span></span><br><span class="line">    X_std = np.zeros(X.shape)</span><br><span class="line">    mean = X.mean(axis=<span class="number">0</span>)</span><br><span class="line">    std = X.std(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 做除法运算时请永远记住分母不能等于0的情形</span></span><br><span class="line">    <span class="comment"># X_std = (X - X.mean(axis=0)) / X.std(axis=0) </span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> range(np.shape(X)[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> std[col]:</span><br><span class="line">            X_std[:, col] = (X_std[:, col] - mean[col]) / std[col]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_std</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分数据集为训练集和测试集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_test_split</span><span class="params">(X, y, test_size=<span class="number">0.2</span>, shuffle=True, seed=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        X, y = shuffle_data(X, y, seed)</span><br><span class="line"></span><br><span class="line">    n_train_samples = int(X.shape[<span class="number">0</span>] * (<span class="number">1</span>-test_size))</span><br><span class="line">    x_train, x_test = X[:n_train_samples], X[n_train_samples:]</span><br><span class="line">    y_train, y_test = y[:n_train_samples], y[n_train_samples:]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_train, x_test, y_train, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算矩阵X的协方差矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_covariance_matrix</span><span class="params">(X, Y=np.empty<span class="params">(<span class="params">(<span class="number">0</span>,<span class="number">0</span>)</span>)</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> Y.any():</span><br><span class="line">        Y = X</span><br><span class="line">    n_samples = np.shape(X)[<span class="number">0</span>]</span><br><span class="line">    covariance_matrix = (<span class="number">1</span> / (n_samples<span class="number">-1</span>)) * (X - X.mean(axis=<span class="number">0</span>)).T.dot(Y - Y.mean(axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.array(covariance_matrix, dtype=float)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算数据集X每列的方差</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_variance</span><span class="params">(X)</span>:</span></span><br><span class="line">    n_samples = np.shape(X)[<span class="number">0</span>]</span><br><span class="line">    variance = (<span class="number">1</span> / n_samples) * np.diag((X - X.mean(axis=<span class="number">0</span>)).T.dot(X - X.mean(axis=<span class="number">0</span>)))</span><br><span class="line">    <span class="keyword">return</span> variance</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算数据集X每列的标准差</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_std_dev</span><span class="params">(X)</span>:</span></span><br><span class="line">    std_dev = np.sqrt(calculate_variance(X))</span><br><span class="line">    <span class="keyword">return</span> std_dev</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算相关系数矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_correlation_matrix</span><span class="params">(X, Y=np.empty<span class="params">([<span class="number">0</span>])</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 先计算协方差矩阵</span></span><br><span class="line">    covariance_matrix = calculate_covariance_matrix(X, Y)</span><br><span class="line">    <span class="comment"># 计算X, Y的标准差</span></span><br><span class="line">    std_dev_X = np.expand_dims(calculate_std_dev(X), <span class="number">1</span>)</span><br><span class="line">    std_dev_y = np.expand_dims(calculate_std_dev(Y), <span class="number">1</span>)</span><br><span class="line">    correlation_matrix = np.divide(covariance_matrix, std_dev_X.dot(std_dev_y.T))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.array(correlation_matrix, dtype=float)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PCA</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    主成份分析算法PCA，非监督学习算法.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.eigen_values = <span class="literal">None</span></span><br><span class="line">        self.eigen_vectors = <span class="literal">None</span></span><br><span class="line">        self.k = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" </span></span><br><span class="line"><span class="string">        将原始数据集X通过PCA进行降维</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        covariance = calculate_covariance_matrix(X)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 求解特征值和特征向量</span></span><br><span class="line">        self.eigen_values, self.eigen_vectors = np.linalg.eig(covariance)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将特征值从大到小进行排序，注意特征向量是按列排的，即self.eigen_vectors第k列是self.eigen_values中第k个特征值对应的特征向量</span></span><br><span class="line">        idx = self.eigen_values.argsort()[::<span class="number">-1</span>]</span><br><span class="line">        eigenvalues = self.eigen_values[idx][:self.k]</span><br><span class="line">        eigenvectors = self.eigen_vectors[:, idx][:, :self.k]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将原始数据集X映射到低维空间</span></span><br><span class="line">        X_transformed = X.dot(eigenvectors)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X_transformed</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># Load the dataset</span></span><br><span class="line">    data = datasets.load_iris()</span><br><span class="line">    X = data.data</span><br><span class="line">    y = data.target</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将数据集X映射到低维空间</span></span><br><span class="line">    X_trans = PCA().transform(X)</span><br><span class="line"></span><br><span class="line">    x1 = X_trans[:, <span class="number">0</span>]</span><br><span class="line">    x2 = X_trans[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    cmap = plt.get_cmap(<span class="string">'viridis'</span>)</span><br><span class="line">    colors = [cmap(i) <span class="keyword">for</span> i <span class="keyword">in</span> np.linspace(<span class="number">0</span>, <span class="number">1</span>, len(np.unique(y)))]</span><br><span class="line"></span><br><span class="line">    class_distr = []</span><br><span class="line">    <span class="comment"># Plot the different class distributions</span></span><br><span class="line">    <span class="keyword">for</span> i, l <span class="keyword">in</span> enumerate(np.unique(y)):</span><br><span class="line">        _x1 = x1[y == l]</span><br><span class="line">        _x2 = x2[y == l]</span><br><span class="line">        _y = y[y == l]</span><br><span class="line">        class_distr.append(plt.scatter(_x1, _x2, color=colors[i]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add a legend</span></span><br><span class="line">    plt.legend(class_distr, y, loc=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Axis labels</span></span><br><span class="line">    plt.xlabel(<span class="string">'Principal Component 1'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Principal Component 2'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>


<p>```</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/12/01/PCA%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95/" data-id="ck50nakqy0002c8vjht8qebwk" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-new-Post" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/11/07/new-Post/" class="article-date">
  <time datetime="2019-11-07T10:58:49.000Z" itemprop="datePublished">2019-11-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/11/07/new-Post/">new Post</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>我要这个天下</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/11/07/new-Post/" data-id="ck50nakr10004c8vj3ybj7z1z" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%87%AA%E8%BF%B0/" rel="tag">自述</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-My-Gallery" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/11/07/My-Gallery/" class="article-date">
  <time datetime="2019-11-07T09:40:36.306Z" itemprop="datePublished">2019-11-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/11/07/My-Gallery/">My Gallery</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Part-1-Count-Based-Word-Vectors"><a href="#Part-1-Count-Based-Word-Vectors" class="headerlink" title="Part 1: Count-Based Word Vectors"></a>Part 1: Count-Based Word Vectors</h1><p>Most word vector models start from the following idea:</p>
<p><em>You shall know a word by the company it keeps (<a href="https://en.wikipedia.org/wiki/John_Rupert_Firth" target="_blank" rel="noopener">Firth, J. R. 1957:11</a>)</em></p>
<p>Many word vector implementations are driven by the idea that similar words, i.e., (near) synonyms, will be used in similar contexts. As a result, similar words will often be spoken or written along with a shared subset of words, i.e., contexts. By examining these contexts, we can try to develop embeddings for our words. With this intuition in mind, many “old school” approaches to constructing word vectors relied on word counts. Here we elaborate upon one of those strategies, <em>co-occurrence matrices</em> (for more information, see <a href="http://web.stanford.edu/class/cs124/lec/vectorsemantics.video.pdf" target="_blank" rel="noopener">here</a> or <a href="https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285" target="_blank" rel="noopener">here</a>).</p>
<h2 id="Word-Vectors"><a href="#Word-Vectors" class="headerlink" title="Word Vectors"></a>Word Vectors</h2><p>Word Vectors are often used as a fundamental component for downstream NLP tasks, e.g. question answering, text generation, translation, etc., so it is important to build some intuitions as to their strengths and weaknesses. Here, you will explore two types of word vectors: those derived from <em>co-occurrence matrices</em>, and those derived via <em>word2vec</em>. </p>
<p><strong>Assignment Notes:</strong> Please make sure to save the notebook as you go along. Submission Instructions are located at the bottom of the notebook.</p>
<p><strong>Note on Terminology:</strong> The terms “word vectors” and “word embeddings” are often used interchangeably. The term “embedding” refers to the fact that we are encoding aspects of a word’s meaning in a lower dimensional space. As <a href="https://en.wikipedia.org/wiki/Word_embedding" target="_blank" rel="noopener">Wikipedia</a> states, “<em>conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension</em>“.</p>
<h1 id="Co-Occurrence"><a href="#Co-Occurrence" class="headerlink" title="Co-Occurrence"></a>Co-Occurrence</h1><p>A co-occurrence matrix counts how often things co-occur in some environment. Given some word $w_i$ occurring in the document, we consider the <em>context window</em> surrounding $w_i$. Supposing our fixed window size is $n$, then this is the $n$ preceding and $n$ subsequent words in that document, i.e. words $w_{i-n} \dots w_{i-1}$ and $w_{i+1} \dots w_{i+n}$. We build a <em>co-occurrence matrix</em> $M$, which is a symmetric word-by-word matrix in which $M_{ij}$ is the number of times $w_j$ appears inside $w_i$’s window.</p>
<p><strong>Example: Co-Occurrence with Fixed Window of n=1</strong>:</p>
<p>Document 1: “all that glitters is not gold”</p>
<p>Document 2: “all is well that ends well”</p>
<table>
<thead>
<tr>
<th>*</th>
<th>START</th>
<th>all</th>
<th>that</th>
<th>glitters</th>
<th>is</th>
<th>not</th>
<th>gold</th>
<th>well</th>
<th>ends</th>
<th>END</th>
</tr>
</thead>
<tbody><tr>
<td>START</td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>all</td>
<td>2</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>that</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>glitters</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>is</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>not</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>gold</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>well</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>ends</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>END</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody></table>
<p><strong>Note:</strong> In NLP, we often add START and END tokens to represent the beginning and end of sentences, paragraphs or documents. In thise case we imagine START and END tokens encapsulating each document, e.g., “START All that glitters is not gold END”, and include these tokens in our co-occurrence counts.</p>
<p>The rows (or columns) of this matrix provide one type of word vectors (those based on word-word co-occurrence), but the vectors will be large in general (linear in the number of distinct words in a corpus). Thus, our next step is to run <em>dimensionality reduction</em>. In particular, we will run <em>SVD (Singular Value Decomposition)</em>, which is a kind of generalized <em>PCA (Principal Components Analysis)</em> to select the top $k$ principal components. Here’s a visualization of dimensionality reduction with SVD. In this picture our co-occurrence matrix is $A$ with $n$ rows corresponding to $n$ words. We obtain a full matrix decomposition, with the singular values ordered in the diagonal $S$ matrix, and our new, shorter length-$k$ word vectors in $U_k$.</p>
<p><img src="imgs/svd.png" alt="Picture of an SVD" title="SVD"></p>
<p>This reduced-dimensionality co-occurrence representation preserves semantic relationships between words, e.g. <em>doctor</em> and <em>hospital</em> will be closer than <em>doctor</em> and <em>dog</em>. </p>
<p><strong>Notes:</strong> If you can barely remember what an eigenvalue is, here’s <a href="https://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf" target="_blank" rel="noopener">a slow, friendly introduction to SVD</a>. If you want to learn more thoroughly about PCA or SVD, feel free to check out lectures <a href="https://web.stanford.edu/class/cs168/l/l7.pdf" target="_blank" rel="noopener">7</a>, <a href="http://theory.stanford.edu/~tim/s15/l/l8.pdf" target="_blank" rel="noopener">8</a>, and <a href="https://web.stanford.edu/class/cs168/l/l9.pdf" target="_blank" rel="noopener">9</a> of CS168. These course notes provide a great high-level treatment of these general purpose algorithms. Though, for the purpose of this class, you only need to know how to extract the k-dimensional embeddings by utilizing pre-programmed implementations of these algorithms from the numpy, scipy, or sklearn python packages. In practice, it is challenging to apply full SVD to large corpora because of the memory needed to perform PCA or SVD. However, if you only want the top $k$ vector components for relatively small $k$ — known as <em><a href="https://en.wikipedia.org/wiki/Singular_value_decomposition#Truncated_SVD" target="_blank" rel="noopener">Truncated SVD</a></em> — then there are reasonably scalable techniques to compute those iteratively.</p>
<h3 id="Plotting-Co-Occurrence-Word-Embeddings"><a href="#Plotting-Co-Occurrence-Word-Embeddings" class="headerlink" title="Plotting Co-Occurrence Word Embeddings"></a>Plotting Co-Occurrence Word Embeddings</h3><p>Here, we will be using the Reuters (business and financial news) corpus. If you haven’t run the import cell at the top of this page, please run it now (click it and press SHIFT-RETURN). The corpus consists of 10,788 news documents totaling 1.3 million words. These documents span 90 categories and are split into train and test. For more details, please see <a href="https://www.nltk.org/book/ch02.html" target="_blank" rel="noopener">https://www.nltk.org/book/ch02.html</a>. We provide a <code>read_corpus</code> function below that pulls out only articles from the “crude” (i.e. news articles about oil, gas, etc.) category. The function also adds START and END tokens to each of the documents, and lowercases words. You do <strong>not</strong> have perform any other kind of pre-processing.</p>
<h3 id="Question-1-1-Implement-distinct-words-code-2-points"><a href="#Question-1-1-Implement-distinct-words-code-2-points" class="headerlink" title="Question 1.1: Implement distinct_words [code] (2 points)"></a>Question 1.1: Implement <code>distinct_words</code> [code] (2 points)</h3><p>Write a method to work out the distinct words (word types) that occur in the corpus. You can do this with <code>for</code> loops, but it’s more efficient to do it with Python list comprehensions. In particular, <a href="https://coderwall.com/p/rcmaea/flatten-a-list-of-lists-in-one-line-in-python" target="_blank" rel="noopener">this</a> may be useful to flatten a list of lists. If you’re not familiar with Python list comprehensions in general, here’s <a href="https://python-3-patterns-idioms-test.readthedocs.io/en/latest/Comprehensions.html" target="_blank" rel="noopener">more information</a>.</p>
<p>You may find it useful to use <a href="https://www.w3schools.com/python/python_sets.asp" target="_blank" rel="noopener">Python sets</a> to remove duplicate words.</p>
<h3 id="Question-1-2-Implement-compute-co-occurrence-matrix-code-3-points"><a href="#Question-1-2-Implement-compute-co-occurrence-matrix-code-3-points" class="headerlink" title="Question 1.2: Implement compute_co_occurrence_matrix [code] (3 points)"></a>Question 1.2: Implement <code>compute_co_occurrence_matrix</code> [code] (3 points)</h3><p>Write a method that constructs a co-occurrence matrix for a certain window-size $n$ (with a default of 4), considering words $n$ before and $n$ after the word in the center of the window. Here, we start to use <code>numpy (np)</code> to represent vectors, matrices, and tensors. If you’re not familiar with NumPy, there’s a NumPy tutorial in the second half of this cs231n <a href="http://cs231n.github.io/python-numpy-tutorial/" target="_blank" rel="noopener">Python NumPy tutorial</a>.</p>
<h3 id="Question-1-3-Implement-reduce-to-k-dim-code-1-point"><a href="#Question-1-3-Implement-reduce-to-k-dim-code-1-point" class="headerlink" title="Question 1.3: Implement reduce_to_k_dim [code] (1 point)"></a>Question 1.3: Implement <code>reduce_to_k_dim</code> [code] (1 point)</h3><p>Construct a method that performs dimensionality reduction on the matrix to produce k-dimensional embeddings. Use SVD to take the top k components and produce a new matrix of k-dimensional embeddings. </p>
<p><strong>Note:</strong> All of numpy, scipy, and scikit-learn (<code>sklearn</code>) provide <em>some</em> implementation of SVD, but only scipy and sklearn provide an implementation of Truncated SVD, and only sklearn provides an efficient randomized algorithm for calculating large-scale Truncated SVD. So please use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html" target="_blank" rel="noopener">sklearn.decomposition.TruncatedSVD</a>.</p>
<h3 id="Question-1-4-Implement-plot-embeddings-code-1-point"><a href="#Question-1-4-Implement-plot-embeddings-code-1-point" class="headerlink" title="Question 1.4: Implement plot_embeddings [code] (1 point)"></a>Question 1.4: Implement <code>plot_embeddings</code> [code] (1 point)</h3><p>Here you will write a function to plot a set of 2D vectors in 2D space. For graphs, we will use Matplotlib (<code>plt</code>).</p>
<p>For this example, you may find it useful to adapt <a href="https://www.pythonmembers.club/2018/05/08/matplotlib-scatter-plot-annotate-set-text-at-label-each-point/" target="_blank" rel="noopener">this code</a>. In the future, a good way to make a plot is to look at <a href="https://matplotlib.org/gallery/index.html" target="_blank" rel="noopener">the Matplotlib gallery</a>, find a plot that looks somewhat like what you want, and adapt the code they give.</p>
<h3 id="Question-1-5-Co-Occurrence-Plot-Analysis-written-3-points"><a href="#Question-1-5-Co-Occurrence-Plot-Analysis-written-3-points" class="headerlink" title="Question 1.5: Co-Occurrence Plot Analysis [written] (3 points)"></a>Question 1.5: Co-Occurrence Plot Analysis [written] (3 points)</h3><p>Now we will put together all the parts you have written! We will compute the co-occurrence matrix with fixed window of 4, over the Reuters “crude” corpus. Then we will use TruncatedSVD to compute 2-dimensional embeddings of each word. TruncatedSVD returns U*S, so we normalize the returned vectors, so that all the vectors will appear around the unit circle (therefore closeness is directional closeness). <strong>Note</strong>: The line of code below that does the normalizing uses the NumPy concept of <em>broadcasting</em>. If you don’t know about broadcasting, check out<br><a href="https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html" target="_blank" rel="noopener">Computation on Arrays: Broadcasting by Jake VanderPlas</a>.</p>
<p>Run the below cell to produce the plot. It’ll probably take a few seconds to run. What clusters together in 2-dimensional embedding space? What doesn’t cluster together that you might think should have?  <strong>Note:</strong> “bpd” stands for “barrels per day” and is a commonly used abbreviation in crude oil topic articles.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/11/07/My-Gallery/" data-id="ck50nakrl0008c8vj2oiw3bps" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/11/04/hello-world/" class="article-date">
  <time datetime="2019-11-04T13:04:40.999Z" itemprop="datePublished">2019-11-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/11/04/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/11/04/hello-world/" data-id="ck50nakqz0003c8vj1mn596fb" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%87%AA%E8%BF%B0/" rel="tag">自述</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/%E8%87%AA%E8%BF%B0/" style="font-size: 10px;">自述</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/01/06/python%E5%AD%A6%E4%B9%A0/hello-world/">高阶函数</a>
          </li>
        
          <li>
            <a href="/2020/01/05/python%E5%AD%A6%E4%B9%A0/%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7/">高级特性</a>
          </li>
        
          <li>
            <a href="/2019/12/03/MDS%E9%99%8D%E7%BB%B4/">MDS</a>
          </li>
        
          <li>
            <a href="/2019/12/01/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96%E7%AE%97%E6%B3%95/">关键词提取</a>
          </li>
        
          <li>
            <a href="/2019/12/01/KPCA%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%20/">KPCA</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>